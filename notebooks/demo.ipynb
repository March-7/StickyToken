{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodlÂÜÖÁΩÆÂ≠¶ÊúØËµÑÊ∫êÂä†ÈÄü\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "os.environ['HF_HOME'] = '/root/autodl-tmp/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append('/root/StickyToken')\n",
    "import torch\n",
    "import gradio as gr\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from stickytoken.utils import load_verification_results\n",
    "from collections import namedtuple\n",
    "DistanceMetrics = namedtuple(\"Metrics\", [\"cosine_distance\", \"euclidean_distance\", \"manhattan_distance\"])\n",
    "from stickytoken.utils import distance_metrics, random_insert\n",
    "from stickytoken.sentence_pair import output_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/StickyToken/results/experiment_information.json\"\n",
    "try:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        experiement_record = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    experiement_record = []\n",
    "experiement_record_df = pd.DataFrame(experiement_record)\n",
    "model_list = experiement_record_df[\"model_name\"].tolist()\n",
    "experiement_record_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\",\n",
    "    \"sup-simcse-bert-base-uncased\", \"sup-simcse-bert-large-uncased\", \"sup-simcse-roberta-base\", \"sup-simcse-roberta-large\",\n",
    "    \"sentence-t5-base\", \"sentence-t5-large\", \"sentence-t5-xl\", \"sentence-t5-xxl\",\n",
    "    \"gtr-t5-base\", \"gtr-t5-large\", \"gtr-t5-xl\", \"gtr-t5-xxl\",\n",
    "    \"instructor-base\", \"instructor-large\", \"instructor-xl\",\n",
    "    \"e5-small\", \"e5-base\", \"e5-large\", \"e5-mistral-7b-instruct\",\n",
    "    \"bge-small-en-v1.5\", \"bge-base-en-v1.5\", \"bge-large-en-v1.5\",\n",
    "    \"UAE-Large-V1\",\n",
    "    \"nomic-embed-text-v1\", \"nomic-embed-text-v1.5\",\n",
    "    \"gte-small\", \"gte-base\", \"gte-large\", \"gte-base-en-v1.5\", \"gte-large-en-v1.5\", \"gte-Qwen2-1.5B-instruct\", \"gte-Qwen2-7B-instruct\",\n",
    "    \"GritLM-7B\",\n",
    "    \"SFR-Embedding-2_R\", \"SFR-Embedding-Mistral\",\n",
    "]\n",
    "# ÊåâÁÖßmodel_namesÁöÑÈ°∫Â∫èÂØπDataFrameËøõË°åÊéíÂ∫è\n",
    "experiement_record_df = experiement_record_df.set_index('model_name').loc[model_names].reset_index()\n",
    "experiement_record_df[['model_name', 'num_parameters', 'vocab_size']].assign(\n",
    "    num_parameters=lambda x: (x['num_parameters'] / 1e6).round().astype(int).astype(str) + 'M'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 250\n",
    "# model_name = model_list[0]\n",
    "# sampled_sentence_pairs_path = output_dataset_name(model_list[0], 'sampled_sentence_pairs', 'csv')\n",
    "# dataset = load_dataset('csv', data_files=sampled_sentence_pairs_path, split='train')\n",
    "\n",
    "# json_file = os.path.join('/root/StickyToken/magicembed', 'model_record.json')\n",
    "# with open(json_file, 'r') as f:\n",
    "#     records = json.load(f)\n",
    "                \n",
    "#     model_name = model_name  # ÂÅáËÆæmodaÊúâ‰∏Ä‰∏™Â±ûÊÄßmodel_name\n",
    "#     mean_similarity = next((np.float32(record[\"vocab_embeddings_mean_cosine_similarity\"]) for record in records if record['model_name'] == model_name), None)\n",
    "\n",
    "# filtered_dataset = dataset.filter(lambda x: x['similarity'] < mean_similarity)\n",
    "        \n",
    "        \n",
    "# min_similarity = min(filtered_dataset['similarity'])\n",
    "# max_similarity = min_similarity + (mean_similarity - min_similarity) * 0.8\n",
    "#         # Âà§Êñ≠ÊòØÂê¶ÊúâË∂≥Â§üÁöÑÂè•Â≠êÂØπ\n",
    "# if len(filtered_dataset) < num:\n",
    "#         print(f\"Âè™Êúâ {len(filtered_dataset)} ‰∏™Âè•Â≠êÂØπÔºåÊó†Ê≥ïÈááÊ†∑ {num} ‰∏™„ÄÇÂª∫ËÆÆÈááÊ†∑{len(filtered_dataset)/2}„ÄÇÂèØ‰ª•ÈÄÇÂΩìÂ¢ûÂä†SentencePair.get_sampled_sentence_pairs()ÁöÑsample_sizeÂèÇÊï∞ > sample_size * {num/len(filtered_dataset)}\")\n",
    "#         num = len(filtered_dataset)\n",
    "\n",
    "# # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Ê≠•Èïø\n",
    "# similarity_step = (max_similarity - min_similarity) / (num - 1)\n",
    "\n",
    "# # ËøõË°åËøë‰ººÂùáÂåÄÈááÊ†∑\n",
    "# sampled_indices = []\n",
    "# for i in range(num):\n",
    "#     target_similarity = min_similarity + i * similarity_step\n",
    "#     closest_index = np.argsort(np.abs(filtered_dataset['similarity'] - target_similarity))[0]\n",
    "#     sampled_indices.append(closest_index)\n",
    "\n",
    "# sampled_dataset = filtered_dataset.select(sampled_indices)\n",
    "# nested_list = [list(pair) for pair in zip(sampled_dataset['sentence1'], sampled_dataset['sentence2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# verification_results = load_verification_results(model_list[0])\n",
    "# verification_results_df = pd.DataFrame(verification_results).transpose()\n",
    "# strong_verified_tokens = verification_results_df[verification_results_df[\"magic\"] == \"strong_verified\"]\n",
    "# magic_token = strong_verified_tokens[\"raw_vocab\"]\n",
    "# magic_token = strong_verified_tokens[\"raw_vocab\"]\n",
    "# magic_token_num = len(magic_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/sentence-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def token_process_pipline(token: str,\n",
    "#                             gt_texts: list[str]|str,\n",
    "#                             contract_texts: list[str]|str,\n",
    "#                             num: int,\n",
    "#                             model,\n",
    "#                             ):\n",
    "#         if isinstance(gt_texts, str):\n",
    "#             gt_texts = [gt_texts]\n",
    "#         if isinstance(contract_texts, str):\n",
    "#             contract_texts = [contract_texts]\n",
    "#         gt_embs = model.encode(gt_texts)\n",
    "#         contract_embs = model.encode(contract_texts)\n",
    "#         gt_metrics = distance_metrics(gt_embs,contract_embs)\n",
    "\n",
    "#         results = {'Prefix': [], 'Suffix': [], 'Insert': []}\n",
    "\n",
    "#         def process_texts(text_list, gt_emb, gt_metric, text_type, id):\n",
    "#             pred_emb = model.encode(text_list)\n",
    "            \n",
    "#             ret = distance_metrics(gt_emb, pred_emb)\n",
    "\n",
    "#             result = {\n",
    "#                 'Pair_id': id,  \n",
    "#                 'Source text': gt_texts[id],\n",
    "#                 'Texts to be contrasted': [contract_text] + text_list,\n",
    "#                 'cosine_distance': [gt_metric.cosine_distance] + list(ret.cosine_distance),\n",
    "#                 'cosine_distance_contrast': [None],\n",
    "#                 'euclidean_distance': [gt_metric.euclidean_distance] + list(ret.euclidean_distance),\n",
    "#                 'euclidean_distance_contrast': [None],\n",
    "#                 'manhattan_distance': [gt_metric.manhattan_distance] + list(ret.manhattan_distance),\n",
    "#                 'manhattan_distance_contrast': [None],\n",
    "#             }\n",
    "\n",
    "#             for i in range(num):\n",
    "#                 cosine_distance_contrast = (result['cosine_distance'][i] > result['cosine_distance'][i + 1])\n",
    "#                 result['cosine_distance_contrast'].append(cosine_distance_contrast)\n",
    "#                 euclidean_distance_contrast = (result['euclidean_distance'][i] > result['euclidean_distance'][i + 1])\n",
    "#                 result['euclidean_distance_contrast'].append(euclidean_distance_contrast)\n",
    "#                 manhattan_distance_contrast = (result['manhattan_distance'][i] > result['manhattan_distance'][i + 1])\n",
    "#                 result['manhattan_distance_contrast'].append(manhattan_distance_contrast)\n",
    "\n",
    "#             results[text_type].append(result)\n",
    "\n",
    "#         for id, contract_text in enumerate(contract_texts):\n",
    "#             text_list_prefix = [token * i + contract_text for i in range(1, num + 1)]\n",
    "#             text_list_suffix = [contract_text + token * i for i in range(1, num + 1)]\n",
    "#             text_list_insert = []\n",
    "#             tem_text = contract_text\n",
    "#             for i in range(1, num + 1):\n",
    "#                 new_text = random_insert(tem_text, token, 1)\n",
    "#                 tem_text = new_text\n",
    "#                 text_list_insert.append(new_text)\n",
    "            \n",
    "#             for text_list, text_type in [(text_list_prefix, 'Prefix'), (text_list_suffix, 'Suffix'), (text_list_insert, 'Insert')]:\n",
    "#                 process_texts(text_list, gt_embs[id],\n",
    "#                             DistanceMetrics(\n",
    "#                                     gt_metrics.cosine_distance[id],\n",
    "#                                     gt_metrics.euclidean_distance[id],\n",
    "#                                     gt_metrics.manhattan_distance[id]\n",
    "#                                 )\n",
    "#                                 , text_type, id)\n",
    "#         return pd.DataFrame({\n",
    "#         'Prefix_text': results['Prefix'][0]['Texts to be contrasted'],\n",
    "#         'Suffix_text': results['Suffix'][0]['Texts to be contrasted'], \n",
    "#         'Insert-text': results['Insert'][0]['Texts to be contrasted'],\n",
    "#         'Prefix': results['Prefix'][0]['cosine_distance'],\n",
    "#         'Suffix': results['Suffix'][0]['cosine_distance'], \n",
    "#         'Insert': results['Insert'][0]['cosine_distance'],\n",
    "#         'num': range(len(results['Prefix'][0]['cosine_distance']))\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_process_pipline('<s>', [\"hello\"], [\"good morning\"], 8, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = token_process_pipline('<s>', [\"hello\"], [\"good morning\"], 8, model)\n",
    "# result['Prefix'][0]['cosine_distance']\n",
    "# result_df = pd.DataFrame({\n",
    "#     'Prefix': result['Prefix'][0]['cosine_distance'],\n",
    "#     'Suffix': result['Suffix'][0]['cosine_distance'], \n",
    "#     'Insert': result['Insert'][0]['cosine_distance'],\n",
    "#     'num': range(len(result['Prefix'][0]['cosine_distance']))\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ui():\n",
    "\n",
    "    def update_model_path(selected_model_name):\n",
    "        model_path = experiement_record_df.loc[experiement_record_df[\"model_name\"] == selected_model_name, \"model\"].values[0]\n",
    "        return model_path\n",
    "    \n",
    "    model_path = update_model_path(model_list[0])\n",
    "\n",
    "    def update_magic_token(selected_model_name):\n",
    "        verification_results = load_verification_results(selected_model_name)\n",
    "        verification_results_df = pd.DataFrame(verification_results).transpose()\n",
    "        strong_verified_tokens = verification_results_df[verification_results_df[\"magic\"] == \"strong_verified\"]\n",
    "        token_list = strong_verified_tokens[\"raw_vocab\"].tolist()\n",
    "        \n",
    "        if len(token_list)<6:\n",
    "            weak_verified_tokens = verification_results_df[verification_results_df[\"magic\"] == \"weak_verified\"]\n",
    "            token_list = weak_verified_tokens[\"raw_vocab\"].tolist()\n",
    "        \n",
    "        return token_list, len(token_list)\n",
    "    \n",
    "    def update_model(selected_model_name):\n",
    "        torch.cuda.empty_cache()\n",
    "        new_model_path = update_model_path(selected_model_name)\n",
    "        model = SentenceTransformer(str(new_model_path))\n",
    "        return model\n",
    "\n",
    "    def update_examples(model_name):\n",
    "        num = 250\n",
    "        sampled_sentence_pairs_path = output_dataset_name(model_name, 'sampled_sentence_pairs', 'csv')\n",
    "        dataset = load_dataset('csv', data_files=sampled_sentence_pairs_path, split='train')\n",
    "\n",
    "        json_file = os.path.join('/root/StickyToken/magicembed', 'model_record.json')\n",
    "        with open(json_file, 'r') as f:\n",
    "            records = json.load(f)\n",
    "            mean_similarity = next((np.float32(record[\"vocab_embeddings_mean_cosine_similarity\"]) for record in records if record['model_name'] == model_name), None)\n",
    "\n",
    "        filtered_dataset = dataset.filter(lambda x: x['similarity'] < mean_similarity)\n",
    "                \n",
    "        min_similarity = min(filtered_dataset['similarity'])\n",
    "        max_similarity = min_similarity + (mean_similarity - min_similarity) * 0.8\n",
    "                # Âà§Êñ≠ÊòØÂê¶ÊúâË∂≥Â§üÁöÑÂè•Â≠êÂØπ\n",
    "        if len(filtered_dataset) < num:\n",
    "            num = len(filtered_dataset)\n",
    "\n",
    "        # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Ê≠•Èïø\n",
    "        similarity_step = (max_similarity - min_similarity) / (num - 1)\n",
    "\n",
    "        # ËøõË°åËøë‰ººÂùáÂåÄÈááÊ†∑\n",
    "        sampled_indices = []\n",
    "        for i in range(num):\n",
    "            target_similarity = min_similarity + i * similarity_step\n",
    "            closest_index = np.argsort(np.abs(filtered_dataset['similarity'] - target_similarity))[0]\n",
    "            sampled_indices.append(closest_index)\n",
    "\n",
    "        sampled_dataset = filtered_dataset.select(sampled_indices)\n",
    "        nested_list = [list(pair) for pair in zip(sampled_dataset['sentence1'], sampled_dataset['sentence2'])]\n",
    "        random.shuffle(nested_list)\n",
    "        return gr.Dataset(samples=nested_list)\n",
    "\n",
    "    def init_examples(model_name):\n",
    "            num = 250\n",
    "            sampled_sentence_pairs_path = output_dataset_name(model_name, 'sampled_sentence_pairs', 'csv')\n",
    "            dataset = load_dataset('csv', data_files=sampled_sentence_pairs_path, split='train')\n",
    "\n",
    "            json_file = os.path.join('/root/StickyToken/magicembed', 'model_record.json')\n",
    "            with open(json_file, 'r') as f:\n",
    "                records = json.load(f)\n",
    "                mean_similarity = next((np.float32(record[\"vocab_embeddings_mean_cosine_similarity\"]) for record in records if record['model_name'] == model_name), None)\n",
    "\n",
    "            filtered_dataset = dataset.filter(lambda x: x['similarity'] < mean_similarity)\n",
    "                    \n",
    "            min_similarity = min(filtered_dataset['similarity'])\n",
    "            max_similarity = min_similarity + (mean_similarity - min_similarity) * 0.8\n",
    "                    # Âà§Êñ≠ÊòØÂê¶ÊúâË∂≥Â§üÁöÑÂè•Â≠êÂØπ\n",
    "            if len(filtered_dataset) < num:\n",
    "                num = len(filtered_dataset)\n",
    "\n",
    "            # ËÆ°ÁÆóÁõ∏‰ººÂ∫¶Ê≠•Èïø\n",
    "            similarity_step = (max_similarity - min_similarity) / (num - 1)\n",
    "\n",
    "            # ËøõË°åËøë‰ººÂùáÂåÄÈááÊ†∑\n",
    "            sampled_indices = []\n",
    "            for i in range(num):\n",
    "                target_similarity = min_similarity + i * similarity_step\n",
    "                closest_index = np.argsort(np.abs(filtered_dataset['similarity'] - target_similarity))[0]\n",
    "                sampled_indices.append(closest_index)\n",
    "\n",
    "            sampled_dataset = filtered_dataset.select(sampled_indices)\n",
    "            nested_list = [list(pair) for pair in zip(sampled_dataset['sentence1'], sampled_dataset['sentence2'])]\n",
    "            random.shuffle(nested_list)\n",
    "            return nested_list\n",
    "\n",
    "    model = update_model(model_list[0])\n",
    "\n",
    "    def token_process_pipline(token: str,\n",
    "                              gt_texts: list[str] | str,\n",
    "                              contract_texts: list[str] | str,\n",
    "                              num: int,\n",
    "                              model=model):\n",
    "        if isinstance(gt_texts, str):\n",
    "            gt_texts = [gt_texts]\n",
    "        if isinstance(contract_texts, str):\n",
    "            contract_texts = [contract_texts]\n",
    "        gt_embs = model.encode(gt_texts)\n",
    "        contract_embs = model.encode(contract_texts)\n",
    "        gt_metrics = distance_metrics(gt_embs, contract_embs)\n",
    "\n",
    "        results = {'Prefix': [], 'Suffix': [], 'Insert': []}\n",
    "\n",
    "        def process_texts(text_list, gt_emb, gt_metric, text_type, id):\n",
    "            pred_emb = model.encode(text_list)\n",
    "            ret = distance_metrics(gt_emb, pred_emb)\n",
    "\n",
    "            result = {\n",
    "                'Pair_id': id,  \n",
    "                'Source text': gt_texts[id],\n",
    "                'Texts to be contrasted': [contract_text] + text_list,\n",
    "                'cosine_distance': [gt_metric.cosine_distance] + list(ret.cosine_distance),\n",
    "                'cosine_distance_contrast': [None],\n",
    "                'euclidean_distance': [gt_metric.euclidean_distance] + list(ret.euclidean_distance),\n",
    "                'euclidean_distance_contrast': [None],\n",
    "                'manhattan_distance': [gt_metric.manhattan_distance] + list(ret.manhattan_distance),\n",
    "                'manhattan_distance_contrast': [None],\n",
    "            }\n",
    "\n",
    "            for i in range(num):\n",
    "                cosine_distance_contrast = (result['cosine_distance'][i] > result['cosine_distance'][i + 1])\n",
    "                result['cosine_distance_contrast'].append(cosine_distance_contrast)\n",
    "                euclidean_distance_contrast = (result['euclidean_distance'][i] > result['euclidean_distance'][i + 1])\n",
    "                result['euclidean_distance_contrast'].append(euclidean_distance_contrast)\n",
    "                manhattan_distance_contrast = (result['manhattan_distance'][i] > result['manhattan_distance'][i + 1])\n",
    "                result['manhattan_distance_contrast'].append(manhattan_distance_contrast)\n",
    "\n",
    "            results[text_type].append(result)\n",
    "\n",
    "        for id, contract_text in enumerate(contract_texts):\n",
    "            text_list_prefix = [token * i + contract_text for i in range(1, num + 1)]\n",
    "            text_list_suffix = [contract_text + token * i for i in range(1, num + 1)]\n",
    "            text_list_insert = []\n",
    "            tem_text = contract_text\n",
    "            for i in range(1, num + 1):\n",
    "                new_text = random_insert(tem_text, token, 1)\n",
    "                tem_text = new_text\n",
    "                text_list_insert.append(new_text)\n",
    "            \n",
    "            for text_list, text_type in [(text_list_prefix, 'Prefix'), (text_list_suffix, 'Suffix'), (text_list_insert, 'Insert')]:\n",
    "                process_texts(text_list, gt_embs[id],\n",
    "                              DistanceMetrics(\n",
    "                                  gt_metrics.cosine_distance[id],\n",
    "                                  gt_metrics.euclidean_distance[id],\n",
    "                                  gt_metrics.manhattan_distance[id]\n",
    "                              ), text_type, id)\n",
    "        return pd.DataFrame({\n",
    "            'num': range(len(results['Prefix'][0]['cosine_distance'])),\n",
    "            'Prefix_text': results['Prefix'][0]['Texts to be contrasted'],\n",
    "            'Prefix': [round(x, 2) for x in results['Prefix'][0]['cosine_distance']],\n",
    "            'Suffix_text': results['Suffix'][0]['Texts to be contrasted'], \n",
    "            'Suffix': [round(x, 2) for x in results['Suffix'][0]['cosine_distance']],\n",
    "            'Insert-text': results['Insert'][0]['Texts to be contrasted'],\n",
    "            'Insert': [round(x, 2) for x in results['Insert'][0]['cosine_distance']]\n",
    "        }).round(2)\n",
    "    \n",
    "    with gr.Column():\n",
    "        model_selector = gr.Dropdown(\n",
    "            choices=model_list,\n",
    "            label=\"Model\",\n",
    "            value=model_list[0],\n",
    "            interactive=True,\n",
    "            show_label=False\n",
    "        )\n",
    "        with gr.Row():\n",
    "            model_size = gr.Textbox(experiement_record_df.loc[experiement_record_df[\"model_name\"] == model_list[0], \"num_parameters\"].values[0], label=\"Model Size\")\n",
    "            magic_token_static = gr.Textbox(update_magic_token(model_list[0])[1], label=\"Magic Token Number\")\n",
    "            vocab_size = gr.Textbox(experiement_record_df.loc[experiement_record_df[\"model_name\"] == model_list[0], \"vocab_size\"].values[0], label=\"Vocab Size\")\n",
    "        magic_token = gr.Textbox(update_magic_token(model_list[0])[0], label=\"Magic Token\",\n",
    "                                 max_lines=7,)\n",
    "\n",
    "\n",
    "    # with gr.Row():\n",
    "    #     with gr.Accordion(\n",
    "    #         \"üîç Expand to see all model descriptions\",\n",
    "    #         open=False,\n",
    "    #         elem_id=\"model_description_accordion\",\n",
    "    #     ):\n",
    "            # model_description_md = models.get_model_description_md(task_type=\"sts\")\n",
    "            # gr.Markdown(model_description_md, elem_id=\"model_description_markdown\")\n",
    "            # gr.Markdown(\"test\")\n",
    "    \n",
    "    empty_df = pd.DataFrame({'num': [], \n",
    "                             'Prefix_text': [],\n",
    "                             'Prefix': [], \n",
    "                             'Suffix_text': [],\n",
    "                             'Suffix': [], \n",
    "                             'Insert-text': [],\n",
    "                             'Insert': [],\n",
    "                            })\n",
    "    result_df = gr.DataFrame(empty_df)\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            token = gr.Textbox(\n",
    "                value=update_magic_token(model_list[0])[0][0],\n",
    "                label=\"Token\",\n",
    "                placeholder=\"Please input magic token\",\n",
    "            )\n",
    "            insrt_num = gr.Slider(value=8, minimum=1, maximum=20, step=1, label=\"Insert Number\")\n",
    "            insert_method = gr.CheckboxGroup(['Prefix', 'Suffix', 'Insert'], label=\"insert_method\", value=['Prefix', 'Suffix', 'Insert'])\n",
    "\n",
    "            gt_text = gr.Textbox(\n",
    "                value=\"hello\",\n",
    "                label=\"Ground Truth Text\",\n",
    "                placeholder=\"Please input ground truth\",\n",
    "            )\n",
    "            contract_texts = gr.Textbox(\n",
    "                value=\"good morning\",\n",
    "                label=\"Contract Texts\",\n",
    "                placeholder=\"Please input contract texts\",\n",
    "            )\n",
    "            with gr.Row():\n",
    "                draw_btn = gr.Button(value=\"üé≤ Random sentences\", variant=\"primary\", scale=1)\n",
    "                draw_token_btn = gr.Button(value=\"üé≤ Random token\", variant=\"primary\", scale=1)\n",
    "            send_btn = gr.Button(value=\"Send\", variant=\"primary\", scale=1)\n",
    "        with gr.Column():\n",
    "            plt_prefix = gr.LinePlot(empty_df, x=\"num\", y=\"Prefix\")\n",
    "            plt_suffix = gr.LinePlot(empty_df, x=\"num\", y=\"Suffix\")\n",
    "            plt_insert = gr.LinePlot(empty_df, x=\"num\", y=\"Insert\")\n",
    "            plots = [plt_prefix, plt_suffix, plt_insert]\n",
    "\n",
    "            def select_region(selection: gr.SelectData):\n",
    "                min_w, max_w = selection.index\n",
    "                return [gr.LinePlot(x_lim=(min_w, max_w))] * len(plots) \n",
    "\n",
    "            for plt in plots:\n",
    "                plt.select(select_region, None, plots)\n",
    "                plt.double_click(lambda: [gr.LinePlot(x_lim=None)] * len(plots), None, plots)\n",
    "\n",
    "    example = gr.Examples(\n",
    "        examples=init_examples(model_list[0]),\n",
    "        inputs=[gt_text, contract_texts],\n",
    "    )\n",
    "\n",
    "    model_selector.change(\n",
    "        fn=update_model_path, inputs=model_selector, outputs=None\n",
    "    ).then(\n",
    "        fn=lambda x: experiement_record_df.loc[experiement_record_df[\"model_name\"] == x, \"num_parameters\"].values[0],\n",
    "        inputs=model_selector,\n",
    "        outputs=model_size\n",
    "    ).then(\n",
    "        fn=lambda x: experiement_record_df.loc[experiement_record_df[\"model_name\"] == x, \"vocab_size\"].values[0],\n",
    "        inputs=model_selector,\n",
    "        outputs=vocab_size\n",
    "    ).then(\n",
    "        fn=update_magic_token, inputs=model_selector,\n",
    "        outputs=[magic_token, magic_token_static]\n",
    "    ).then(\n",
    "        fn=update_examples, inputs=model_selector,\n",
    "        outputs=example.dataset\n",
    "    ).success(\n",
    "        fn=update_model, inputs=model_selector, outputs=None\n",
    "    )\n",
    "\n",
    "    draw_btn.click(\n",
    "        fn=lambda: random.choice(example.examples),\n",
    "        inputs=None,\n",
    "        outputs=[gt_text, contract_texts]\n",
    "    )\n",
    "\n",
    "    draw_token_btn.click(\n",
    "        fn=lambda x: random.choice(update_magic_token(x)[0]),\n",
    "        inputs=model_selector,\n",
    "        outputs=token\n",
    "    )\n",
    "\n",
    "    send_btn.click(\n",
    "        fn=token_process_pipline,\n",
    "        inputs=[token, gt_text, contract_texts, insrt_num],\n",
    "        outputs=result_df,\n",
    "    ).then(\n",
    "        fn=lambda df: [gr.LinePlot(df, x=\"num\", y=col) for col in [\"Prefix\", \"Suffix\", \"Insert\"]],\n",
    "        inputs=result_df,\n",
    "        outputs=plots\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks(title=\"Magic Token\") as block:\n",
    "    notice_markdown = f\"\"\"\n",
    "# üíß Magic Token ‚òòÔ∏è\n",
    "\"\"\"\n",
    "    gr.Markdown(notice_markdown, elem_id=\"notice_markdown\")\n",
    "\n",
    "    gr.Markdown(\"Adding these tokens into sentence pairs results in atypical changes to the similarity of their text embeddings. We conduct an exhaustive analysis of the tokenizer mechanisms in text embedding models, focusing on identifying these ‚Äúmagic tokens‚Äù. By leveraging tokenizer examination, indicators, and validation procedures, we develop effective methods for automatically detecting these problematic tokens.\")\n",
    "\n",
    "    with gr.Tab(\"‚òòÔ∏è Model\"):\n",
    "        build_model_ui()\n",
    "    \n",
    "    with gr.Tab(\"üîç Table\"):\n",
    "        # Â±ïÁ§∫experiement_record_df,ÂéªÈô§‰∏çÈúÄË¶ÅÁöÑÂàó\n",
    "        gr.DataFrame(experiement_record_df.drop(columns=[\"record_time\", \"model\",\"dataset\",\"insert_num\",\"sent_pair_num\",\"verification_sent_pair_num\",\"vocab_embeddings_is_on_unit_sphere\",\n",
    "        \"wte_is_on_unit_sphere\",\n",
    "        \"vocab_embeddings_is_anisotropic\",\n",
    "        \"wte_is_anisotropic\",]).round(2))\n",
    "\n",
    "block.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stickytoken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
