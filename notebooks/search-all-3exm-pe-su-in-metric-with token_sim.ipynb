{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0947a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EXP:\n",
    "    MODEL = 'sentence-transformers/sentence-t5-base'\n",
    "    DATA = '/root/StickyToken/data/sampled_df.csv'\n",
    "    VERIFICATION_DATA = '/root/StickyToken/data/sampled_df_not-pair.csv'\n",
    "    SENT_PAIR_NUM = 5\n",
    "    INSERT_NUM = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea69c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T09:43:54.821654Z",
     "iopub.status.busy": "2024-04-20T09:43:54.821365Z",
     "iopub.status.idle": "2024-04-20T09:44:08.711641Z",
     "shell.execute_reply": "2024-04-20T09:44:08.710812Z"
    },
    "papermill": {
     "duration": 13.901812,
     "end_time": "2024-04-20T09:44:08.714010",
     "exception": false,
     "start_time": "2024-04-20T09:43:54.812198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "import sys\n",
    "sys.path.append('/root/StickyToken')\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity,cosine_distances, euclidean_distances, manhattan_distances\n",
    "from collections import Counter, namedtuple\n",
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "import anndata\n",
    "import warnings\n",
    "import pynvml\n",
    "import json\n",
    "from time import time\n",
    "import jsonlines\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f403c8",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-20T09:44:08.731220Z",
     "iopub.status.busy": "2024-04-20T09:44:08.730684Z",
     "iopub.status.idle": "2024-04-20T09:44:15.547496Z",
     "shell.execute_reply": "2024-04-20T09:44:15.546674Z"
    },
    "papermill": {
     "duration": 6.827716,
     "end_time": "2024-04-20T09:44:15.549922",
     "exception": false,
     "start_time": "2024-04-20T09:44:08.722206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(EXP.MODEL)\n",
    "transformer_model = model._first_module().auto_model\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05935237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stickytoken.tokenization import TokenizerAnalyzer\n",
    "toka = TokenizerAnalyzer(EXP.MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b6bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_info = toka.categorize_tokens()\n",
    "token_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67af9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计category属性\n",
    "category_counts = {}\n",
    "for token in token_info.values():\n",
    "    category = token['category']\n",
    "    if category in category_counts:\n",
    "        category_counts[category] += 1\n",
    "    else:\n",
    "        category_counts[category] = 1\n",
    "\n",
    "# 打印统计结果\n",
    "print(\"Category统计结果:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "\n",
    "# 计算百分比\n",
    "total_tokens = len(token_info)\n",
    "print(\"\\nCategory百分比:\")\n",
    "for category, count in category_counts.items():\n",
    "    percentage = (count / total_tokens) * 100\n",
    "    print(f\"{category}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找category属性为'OK_SPECIAL'的所有元素\n",
    "ok_special_tokens = {k: v for k, v in token_info.items() if v['category'] == 'OK_SPECIAL'}\n",
    "\n",
    "print(\"OK_SPECIAL类别的元素:\")\n",
    "for token_id, token_data in ok_special_tokens.items():\n",
    "    print(f\"Token ID: {token_id}\")\n",
    "    print(f\"Raw Vocab: {token_data['raw_vocab']}\")\n",
    "    print(f\"Decoded: {token_data['decoded']}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"OK_SPECIAL类别的元素总数: {len(ok_special_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bfe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找category属性为'UNREACHABLE_MULTI_TOKEN'的所有元素\n",
    "unreachable_multi_tokens = {k: v for k, v in token_info.items() if v['category'] == 'UNREACHABLE_MULTI_TOKEN'}\n",
    "\n",
    "print(\"UNREACHABLE_MULTI_TOKEN类别的元素:\")\n",
    "for token_id, token_data in unreachable_multi_tokens.items():\n",
    "    print(f\"Token ID: {token_id}\")\n",
    "    print(f\"Raw Vocab: {token_data['raw_vocab']}\")\n",
    "    print(f\"Decoded: {token_data['decoded']}\")\n",
    "    print(f\"Reencoded IDs: {token_data.get('reencoded_ids', 'N/A')}\")\n",
    "    print(f\"Reencoded: {token_data.get('reencoded', 'N/A')}\")\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"UNREACHABLE_MULTI_TOKEN类别的元素总数: {len(unreachable_multi_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd64273",
   "metadata": {},
   "outputs": [],
   "source": [
    "DistanceMetrics = namedtuple(\"Metrics\", [\"cosine_distance\", \"euclidean_distance\", \"manhattan_distance\"])\n",
    "\n",
    "def distance_metrics(emb1: np.ndarray,emb2:np.ndarray ) -> DistanceMetrics:\n",
    "    \"\"\"\n",
    "    计算两个嵌入向量之间的距离度量。\n",
    "\n",
    "    参数:\n",
    "    emb1 (np.ndarray): 第一个嵌入向量或嵌入向量矩阵\n",
    "    emb2 (np.ndarray): 第二个嵌入向量或嵌入向量矩阵\n",
    "\n",
    "    返回:\n",
    "    DistanceMetrics: 包含余弦距离、欧几里得距离和曼哈顿距离的命名元组\n",
    "\n",
    "    注意:\n",
    "    - 如果emb1是1维向量而emb2是2维矩阵，函数会将emb1重塑为2维\n",
    "    - 如果两个输入都是2维矩阵，函数会计算对角线上的距离\n",
    "    \"\"\"\n",
    "    if emb1.ndim == 1 and emb2.ndim != 1:\n",
    "        emb1 = emb1.reshape(1, -1)\n",
    "        return DistanceMetrics(\n",
    "            cosine_distances(emb1, emb2)[0],\n",
    "            euclidean_distances(emb1, emb2)[0],\n",
    "            manhattan_distances(emb1, emb2)[0],\n",
    "        )\n",
    "    elif emb1.ndim != 1 and emb2.ndim != 1:\n",
    "        return DistanceMetrics(\n",
    "            cosine_distances(emb1, emb2).diagonal(),\n",
    "            euclidean_distances(emb1, emb2).diagonal(),\n",
    "            manhattan_distances(emb1, emb2).diagonal(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc8b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files=EXP.DATA,split='train')\n",
    "gt_texts = dataset['sentence1'][:EXP.SENT_PAIR_NUM]\n",
    "gt_embs = model.encode(gt_texts)\n",
    "contract_texts = dataset['sentence2'][:EXP.SENT_PAIR_NUM]\n",
    "contract_embs = model.encode(contract_texts)\n",
    "# gt_cs = cosine_similarity(gt_embs, contract_embs).diagonal()\n",
    "# print(gt_embs.shape)\n",
    "gt_metrics = distance_metrics(gt_embs, contract_embs)\n",
    "print(gt_embs.shape)\n",
    "print(contract_embs.shape)\n",
    "print(gt_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_dataset = load_dataset('csv', data_files=EXP.VERIFICATION_DATA,split='train')\n",
    "verification_gt_texts = verification_dataset['sentence1']\n",
    "verification_contract_texts = verification_dataset['sentence2']\n",
    "verification_gt_embs = model.encode(verification_gt_texts)\n",
    "print(verification_gt_embs.shape)\n",
    "verification_contract_embs = model.encode(verification_contract_texts)\n",
    "print(verification_contract_embs.shape)\n",
    "verification_gt_metrics = distance_metrics(verification_gt_embs, verification_contract_embs)\n",
    "# print(verification_gt_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c30b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算单个token与所有gt_texts的余弦相似度\n",
    "def calculate_token_distances(token, gt_embs, model):\n",
    "    \"\"\"\n",
    "    计算单个token与所有gt_texts的余弦距离、欧氏距离和曼哈顿距离\n",
    "    \n",
    "    参数:\n",
    "    token (str): 要计算距离的token\n",
    "    gt_embss (list): 所有的ground truth文本列表的嵌入表示\n",
    "    model: 用于编码的模型\n",
    "    \n",
    "    返回:\n",
    "    tuple: 包含三个np.array，分别是token与每个gt_text的余弦距离、欧氏距离和曼哈顿距离\n",
    "    \"\"\"\n",
    "    # 编码token\n",
    "    # token_emb = model.encode(token)   #(768,)\n",
    "    token_emb = model.encode([token])  #(1,768)\n",
    "    print(token_emb.shape)\n",
    "    print(gt_embs.shape)\n",
    "    # 计算token与所有gt_texts的余弦距离、欧氏距离和曼哈顿距离\n",
    "    cosine_distance = cosine_distances(token_emb, gt_embs)[0]\n",
    "    euclidean_distance = euclidean_distances(token_emb, gt_embs)[0]\n",
    "    manhattan_distance = manhattan_distances(token_emb, gt_embs)[0]\n",
    "    \n",
    "    return DistanceMetrics(cosine_distance=cosine_distance,\n",
    "                           euclidean_distance=euclidean_distance,\n",
    "                           manhattan_distance=manhattan_distance)\n",
    "\n",
    "# 示例使用\n",
    "token = tokenizer.convert_ids_to_tokens(6182)\n",
    "token_distances = calculate_token_distances(token, gt_embs, model)\n",
    "\n",
    "print(f\"Token '{token}' 与所有gt_texts的距离:\")\n",
    "print(token_distances)\n",
    "print(f\"平均距离: {np.mean(token_distances.cosine_distance):.4f}\")\n",
    "print(f\"最大距离: {np.max(token_distances.cosine_distance):.4f}\")\n",
    "print(f\"最小距离: {np.min(token_distances.cosine_distance):.4f}\")\n",
    "print(f\"欧氏距离平均值: {np.mean(token_distances.euclidean_distance):.4f}\")\n",
    "print(f\"曼哈顿距离平均值: {np.mean(token_distances.manhattan_distance):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198ee04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('egg',add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2f9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wte = transformer_model.encoder.embed_tokens.weight\n",
    "    wte = wte.detach().cpu().numpy()\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    wte = wte[0:vocab_size]\n",
    "except:\n",
    "    print('无法获取权重')\n",
    "    pass \n",
    "print(wte.shape)\n",
    "# ad = sc.AnnData(wte)[0:vocab_size]\n",
    "# ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5870fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = [tokenizer.convert_ids_to_tokens(i) for i in range(vocab_size)]\n",
    "print(all_tokens[:10])\n",
    "all_embeddings = model.encode(all_tokens)\n",
    "print(all_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997a9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(all_embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f04925",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vectors_on_unit_sphere(embeddings):\n",
    "    \"\"\"\n",
    "    检查所有向量是否在单位超球体上\n",
    "\n",
    "    参数:\n",
    "    embeddings (np.ndarray): 所有token的嵌入向量\n",
    "\n",
    "    返回:\n",
    "    bool: 如果所有向量都在单位超球体上，返回True；否则返回False\n",
    "    \"\"\"\n",
    "    # 计算每个向量的模长\n",
    "    vector_norms = np.linalg.norm(embeddings, axis=1)\n",
    "\n",
    "    # 判断是否所有向量的模长都为1\n",
    "    is_on_unit_sphere = np.allclose(vector_norms, 1)\n",
    "    if not is_on_unit_sphere:\n",
    "        # 统计不在单位超球体上的向量数量\n",
    "        num_not_on_unit_sphere = np.sum(~np.isclose(vector_norms, 1))\n",
    "        print(f\"不在单位超球体上的向量数量: {num_not_on_unit_sphere}\")\n",
    "        \n",
    "        # 计算这些向量的模长的平均值和方差\n",
    "        not_on_unit_sphere_norms = vector_norms[~np.isclose(vector_norms, 1)]\n",
    "        mean_norm = np.mean(not_on_unit_sphere_norms)\n",
    "        variance_norm = np.var(not_on_unit_sphere_norms)\n",
    "        print(f\"不在单位超球体上的向量模长的平均值: {mean_norm}\")\n",
    "        print(f\"不在单位超球体上的向量模长的方差: {variance_norm}\")\n",
    "\n",
    "\n",
    "    return is_on_unit_sphere\n",
    "\n",
    "# 使用函数\n",
    "all_embeddings_is_on_unit_sphere = check_vectors_on_unit_sphere(all_embeddings)\n",
    "print(f\"输出侧的所有token的向量是否都在单位超球体上: {all_embeddings_is_on_unit_sphere}\")\n",
    "wte_is_on_unit_sphere = check_vectors_on_unit_sphere(wte)  \n",
    "print(f\"wte中的所有token的权重是否都在单位超球体上: {wte_is_on_unit_sphere}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e694a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings_is_anisotropic(all_embeddings, batch_size=128):\n",
    "    \"\"\"\n",
    "    分析嵌入向量，计算余弦相似度并判断是否具有各向异性\n",
    "\n",
    "    参数:\n",
    "    all_embeddings (np.ndarray): 所有token的嵌入向量\n",
    "    batch_size (int): 批处理大小，默认为128\n",
    "\n",
    "    返回:\n",
    "    bool: 如果所有token的向量具有各向异性，返回True；否则返回False\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm.notebook import tqdm\n",
    "    import numpy as np\n",
    "    # 将嵌入移动到GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    all_embeddings = torch.tensor(all_embeddings).to(device)\n",
    "\n",
    "    # 计算向量之间的模长\n",
    "    vector_norms = torch.norm(all_embeddings, dim=1)\n",
    "    print('vector_norms,向量模长：')\n",
    "    print(vector_norms.shape)\n",
    "    print(vector_norms)\n",
    "    # 计算向量之间的余弦相似度\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in tqdm(range(0, all_embeddings.shape[0], batch_size), desc=\"计算余弦相似度\"):\n",
    "        batch_embeddings = all_embeddings[i:i+batch_size]\n",
    "        batch_norms = vector_norms[i:i+batch_size]\n",
    "        for j in range(batch_embeddings.shape[0]):\n",
    "            cosine_similarity = torch.matmul(batch_embeddings[j], all_embeddings.T) / (batch_norms[j] * vector_norms)\n",
    "            cosine_similarities.append(cosine_similarity.cpu().numpy())\n",
    "\n",
    "    # 将cosine_similarities展平成一个numpy数组\n",
    "    cosine_similarities_np = np.concatenate(cosine_similarities).flatten()\n",
    "    \n",
    "    # 画出余弦距离的分布图和密度估计曲线\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(cosine_similarities_np, bins=50, alpha=0.75, color='blue', edgecolor='black', density=True)\n",
    "    \n",
    "    # 添加密度估计曲线\n",
    "    # import seaborn as sns\n",
    "    # sns.kdeplot(cosine_similarities_np, color='red', linewidth=2)\n",
    "    \n",
    "    plt.title('Distribution of Cosine Similarity between Token Embeddings')\n",
    "    plt.xlabel('Cosine Similarity')\n",
    "    plt.ylabel('Density/Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # 对结果进行统计分析\n",
    "    mean_similarity = np.mean(cosine_similarities_np)\n",
    "    median_similarity = np.median(cosine_similarities_np)\n",
    "    std_deviation = np.std(cosine_similarities_np)\n",
    "    min_similarity = np.min(cosine_similarities_np)\n",
    "    max_similarity = np.max(cosine_similarities_np)\n",
    "\n",
    "    # 打印统计结果\n",
    "    print(f\"平均余弦相似度: {mean_similarity}\")\n",
    "    print(f\"中位数余弦相似度: {median_similarity}\")\n",
    "    print(f\"余弦相似度标准差: {std_deviation}\")\n",
    "    print(f\"最小余弦相似度: {min_similarity}\")\n",
    "    print(f\"最大余弦相似度: {max_similarity}\")\n",
    "\n",
    "    # # 使用Kolmogorov-Smirnov检验来判断cosine_similarities是否为均匀分布\n",
    "    # from scipy.stats import kstest\n",
    "    # from scipy.stats import uniform\n",
    "    # ks_statistic, p_value = kstest(cosine_similarities_np, uniform(loc=0, scale=1).cdf)\n",
    "\n",
    "    # # 如果p值小于0.05，则拒绝原假设，认为cosine_similarities不是均匀分布，即具有各向异性\n",
    "    # is_anisotropic = p_value < 0.05\n",
    "    # 使用快速方法判断cosine_similarities是否为均匀分布,均匀分布的余弦相似度应该在0附近\n",
    "    is_anisotropic = not np.allclose(mean_similarity, 0, atol=0.01)\n",
    "\n",
    "    print(f\"所有token的向量是否具有各向异性: {is_anisotropic}\")\n",
    "\n",
    "    return is_anisotropic,mean_similarity,median_similarity,std_deviation,min_similarity,max_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae161e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anisotropic_output, mean_similarity_output, median_similarity_output, std_deviation_output, min_similarity_output, max_similarity_output = check_embeddings_is_anisotropic(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc530cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_anisotropic_wte, mean_similarity_wte, median_similarity_wte, std_deviation_wte, min_similarity_wte, max_similarity_wte = check_embeddings_is_anisotropic(wte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185bbe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d352f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighbor_distances(embeddings, batch_size=128,mode = 'nearest'):\n",
    "    \"\"\"\n",
    "    计算词表中所有token与其最近邻token之间的距离，使用GPU加速计算\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含余弦距离、欧几里得距离和曼哈顿距离的最小值列表\n",
    "    \"\"\"\n",
    "    distances = {\n",
    "        'cosine': [],\n",
    "        'euclidean': [],\n",
    "        'manhattan': []\n",
    "    }\n",
    "    \n",
    "    # 获取词表中所有token的嵌入表示\n",
    "    # vocab_size = tokenizer.vocab_size\n",
    "    # all_tokens = [tokenizer.convert_ids_to_tokens(i) for i in range(vocab_size)]\n",
    "    # all_embeddings = model.encode(all_tokens, convert_to_tensor=True,batch_size=256)  # 转换为PyTorch张量\n",
    "    # 生成一个简单的all_embeddings例子\n",
    "    # 注意:这只是一个示例,实际的all_embeddings会有更多的token和更高的维度\n",
    "    # all_embeddings = torch.tensor([\n",
    "    #     [0.1, 0.2, 0.3],\n",
    "    #     [0.4, 0.5, 0.6],\n",
    "    #     [0.7, 0.8, 0.9],\n",
    "    #     [1.0, 1.1, 1.2],\n",
    "    #     [1.3, 1.4, 1.5],\n",
    "    #     [1.6, 1.7, 1.8]\n",
    "    # ])\n",
    "    # print(\"示例 all_embeddings 形状:\", all_embeddings.shape)\n",
    "    # print(\"示例 all_embeddings 内容:\\n\", all_embeddings)\n",
    "    \n",
    "    # 注意:这里我们使用了一个小的示例\n",
    "    # 实际的代码应该使用原始的all_embeddings,不要替换它\n",
    "    # 将嵌入移动到GPU\n",
    "    # transformer_model = model._first_module().auto_model\n",
    "    # wte = transformer_model.encoder.embed_tokens.weight\n",
    "    # wte = wte.detach().cpu()[:vocab_size]\n",
    "    # all_embeddings = wte\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # all_embeddings = all_embeddings.to(device)\n",
    "    all_embeddings = torch.tensor(embeddings).to(device)\n",
    "    batch_size = 128  # 可以根据GPU内存调整批处理大小\n",
    "    \n",
    "    for i in tqdm(range(0, all_embeddings.shape[0], batch_size), desc=\"计算最近邻距离\"):\n",
    "        batch = all_embeddings[i:i+batch_size]\n",
    "\n",
    "        # 计算批处理与所有嵌入之间的距离\n",
    "        cosine_dist = 1 - torch.nn.functional.cosine_similarity(batch.unsqueeze(1), all_embeddings.unsqueeze(0), dim=2)  #cosine_dist.shape=[128, 32100]\n",
    "        euclidean_dist = torch.cdist(batch, all_embeddings, p=2)\n",
    "        manhattan_dist = torch.cdist(batch, all_embeddings, p=1)\n",
    "\n",
    "        if mode == 'nearest':\n",
    "            # 将自身距离设为无穷大\n",
    "            # 将当前批次的对角线元素设置为无穷大\n",
    "            for j in range(batch_size):\n",
    "                if i+j < len(all_embeddings):\n",
    "                    cosine_dist[j, i+j] = float('inf')\n",
    "                    euclidean_dist[j, i+j] = float('inf')\n",
    "                    manhattan_dist[j, i+j] = float('inf')\n",
    "            # 找到每个token的最小距离\n",
    "            distances['cosine'].extend(cosine_dist.min(dim=1)[0].cpu().numpy())\n",
    "            distances['euclidean'].extend(euclidean_dist.min(dim=1)[0].cpu().numpy())\n",
    "            distances['manhattan'].extend(manhattan_dist.min(dim=1)[0].cpu().numpy())\n",
    "            # print(cosine_dist.shape)\n",
    "            # print(cosine_dist.cpu().numpy().mean(axis=1).shape)\n",
    "        elif mode == 'mean':\n",
    "            distances['cosine'].extend(list(cosine_dist.cpu().numpy().mean(axis=1)))\n",
    "            distances['euclidean'].extend(list(euclidean_dist.cpu().numpy().mean(axis=1)))\n",
    "            distances['manhattan'].extend(list(manhattan_dist.cpu().numpy().mean(axis=1)))\n",
    "\n",
    "    # 清空GPU内存\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# # 计算最近邻距离\n",
    "# nearest_neighbor_distances = calculate_neighbor_distances(model, tokenizer)\n",
    "nearest_neighbor_distances = calculate_neighbor_distances(all_embeddings)\n",
    "print(nearest_neighbor_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_neighbor_distances = calculate_neighbor_distances(all_embeddings,mode='mean')\n",
    "print(mean_neighbor_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971f52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def calculate_nearest_neighbor_distances_cpu(model, tokenizer):\n",
    "    \"\"\"\n",
    "    计算词表中所有token与其最近邻token之间的距离，使用GPU加速计算\n",
    "    \n",
    "    参数:\n",
    "    model: 用于编码的模型\n",
    "    tokenizer: 分词器\n",
    "    \n",
    "    返回:\n",
    "    dict: 包含余弦距离、欧几里得距离和曼哈顿距离的最小值列表\n",
    "    \"\"\"\n",
    "    distances = {\n",
    "        'cosine': [],\n",
    "        'euclidean': [],\n",
    "        'manhattan': []\n",
    "    }\n",
    "    \n",
    "    # 获取词表中所有token的嵌入表示\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "    all_tokens = [tokenizer.convert_ids_to_tokens(i) for i in range(vocab_size)]\n",
    "    all_embeddings = model.encode(all_tokens)\n",
    "\n",
    "    for i in tqdm(range(len(all_embeddings)), desc=\"计算最近邻距离\"):\n",
    "        # 计算当前token与所有其他token的距离\n",
    "        emb1 = all_embeddings[i].reshape(1, -1)\n",
    "        emb2 = all_embeddings\n",
    "        \n",
    "        cosine_dist = cosine_distances(emb1, emb2)[0]\n",
    "        euclidean_dist = euclidean_distances(emb1, emb2)[0]\n",
    "        manhattan_dist = manhattan_distances(emb1, emb2)[0]\n",
    "        \n",
    "        # 将自身距离设为无穷大\n",
    "        cosine_dist[i] = float('inf')\n",
    "        euclidean_dist[i] = float('inf')\n",
    "        manhattan_dist[i] = float('inf')\n",
    "        \n",
    "        # 找到每个token的最小距离\n",
    "        distances['cosine'].append(np.min(cosine_dist))\n",
    "        distances['euclidean'].append(np.min(euclidean_dist))\n",
    "        distances['manhattan'].append(np.min(manhattan_dist))\n",
    "    \n",
    "    # 不需要清空GPU内存，因为现在使用CPU计算\n",
    "    \n",
    "    return distances\n",
    "\n",
    "nearest_neighbor_distances_cpu = calculate_nearest_neighbor_distances_cpu(model, tokenizer)\n",
    "print(nearest_neighbor_distances_cpu)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b815f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def plot_neighbor_distances(nearest_neighbor_distances,mode = 'nearest'):\n",
    "    \"\"\"\n",
    "    绘制最近邻距离的分布图，包括直方图和核密度估计图\n",
    "\n",
    "    参数:\n",
    "    nearest_neighbor_distances (dict): 包含不同距离类型及其值的字典\n",
    "    \"\"\"\n",
    "    # 设置图表样式\n",
    "    plt.style.use('default')  # 使用默认样式而不是seaborn\n",
    "\n",
    "    # 创建一个2x2的子图布局\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n",
    "    if mode == 'nearest':\n",
    "        fig.suptitle('Nearest Neighbor Distance Distribution', fontsize=16)\n",
    "    elif mode == 'mean':\n",
    "        fig.suptitle('Mean Neighbor Distance Distribution', fontsize=16)\n",
    "    # 扁平化axs数组以便于索引\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    # 为每种距离类型绘制直方图和核密度估计图\n",
    "    for i, (distance_type, values) in enumerate(nearest_neighbor_distances.items()):\n",
    "        # 计算阈值，排除接近0的小值\n",
    "        threshold = np.percentile(values, 0)  # 使用第1百分位数作为阈值\n",
    "        filtered_values = [v for v in values if v > threshold]\n",
    "        \n",
    "        axs[i].hist(filtered_values, bins=50, density=True, alpha=0.7)\n",
    "        axs[i].set_title(f'{distance_type.capitalize()} Distance Distribution')\n",
    "        axs[i].set_xlabel('Distance')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        \n",
    "        # 添加核密度估计曲线\n",
    "        sns.kdeplot(filtered_values, ax=axs[i], color='r')\n",
    "        \n",
    "        # 设置x轴的范围，排除接近0的部分\n",
    "        axs[i].set_xlim(left=threshold)\n",
    "\n",
    "    # 移除多余的子图\n",
    "    # fig.delaxes(axs[3])\n",
    "\n",
    "    # 调整子图之间的间距\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "\n",
    "# 调用函数\n",
    "plot_neighbor_distances(mean_neighbor_distances,mode='mean')\n",
    "plot_neighbor_distances(nearest_neighbor_distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd108f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算均值和标准差\n",
    "for distance_type, values in nearest_neighbor_distances.items():\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    max_value = np.max(values)\n",
    "    print(f\"{distance_type} 距离:\")\n",
    "    print(f\"  均值: {mean:.4f}\")\n",
    "    print(f\"  标准差: {std:.4f}\")\n",
    "    threshold = mean\n",
    "    threshold_name = f\"{distance_type}_threshold\"\n",
    "    locals()[threshold_name] = threshold\n",
    "    print(threshold_name)\n",
    "    print(f\"  阈值 (均值 + 标准差): {threshold:.4f}\")\n",
    "    print(f\"  最大值: {max_value:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4796fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f46f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# def compare_token_embeddings(token, wte, tokenizer, model):\n",
    "#     \"\"\"\n",
    "#     比较词表中某个token对应的wte中的向量和model.encode([token])之后的向量是否相同\n",
    "    \n",
    "#     参数:\n",
    "#     token (str): 要比较的token\n",
    "#     wte (numpy.ndarray): 词嵌入矩阵\n",
    "#     tokenizer: 分词器\n",
    "#     model: 用于编码的模型\n",
    "    \n",
    "#     返回:\n",
    "#     bool: 两个向量是否相同\n",
    "#     float: 两个向量的余弦相似度\n",
    "#     \"\"\"\n",
    "#     # 获取token的ID\n",
    "#     token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "#     # print(token_id)\n",
    "#     # 从wte中获取对应的向量\n",
    "#     wte_vector = wte[token_id]\n",
    "#     # print(wte_vector.shape)\n",
    "\n",
    "#     # 使用model.encode获取向量\n",
    "#     encoded_vector = model.encode([token])[0]\n",
    "#     # print(encoded_vector.shape)\n",
    "\n",
    "#     # 计算余弦相似度\n",
    "#     similarity = cosine_similarity([wte_vector], [encoded_vector])[0][0]\n",
    "    \n",
    "#     # 判断两个向量是否相同（考虑到浮点数精度，使用近似相等）\n",
    "#     is_same = np.allclose(wte_vector, encoded_vector, rtol=1e-5, atol=1e-8)\n",
    "    \n",
    "#     return is_same, similarity\n",
    "\n",
    "# # 示例使用\n",
    "# token_to_compare = tokenizer.convert_ids_to_tokens(32073)\n",
    "# is_same, similarity = compare_token_embeddings(token_to_compare, wte, tokenizer, model)\n",
    "\n",
    "# print(f\"Token '{token_to_compare}':\")\n",
    "# print(f\"wte向量和model.encode()向量是否相同: {is_same}\")\n",
    "# print(f\"两个向量的余弦相似度: {similarity}\")\n",
    "# # 统计所有词表中的token\n",
    "\n",
    "# # 初始化结果列表\n",
    "# results = []\n",
    "\n",
    "# # 遍历词表中的所有token\n",
    "# for token_id in tqdm(range(vocab_size), desc=\"处理词表\", unit=\"token\"):\n",
    "#     token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "#     is_same, similarity = compare_token_embeddings(token, wte, tokenizer, model)\n",
    "    \n",
    "#     results.append({\n",
    "#         'token_id': token_id,\n",
    "#         'token': token,\n",
    "#         'is_same': is_same,\n",
    "#         'similarity': similarity\n",
    "#     })\n",
    "\n",
    "\n",
    "# df_results = pd.DataFrame(results)\n",
    "\n",
    "# # 打印统计信息\n",
    "# print(\"统计结果:\")\n",
    "# print(f\"总token数: {len(df_results)}\")\n",
    "# print(f\"wte向量和model.encode()向量相同的token数: {df_results['is_same'].sum()}\")\n",
    "# print(f\"平均余弦相似度: {df_results['similarity'].mean():.4f}\")\n",
    "\n",
    "# # 显示前几行结果\n",
    "# print(\"\\n前5行结果:\")\n",
    "# print(df_results.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e1687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insert(text, insert_string, times):\n",
    "    words = text.split()  # 将句子分割成单词列表\n",
    "    for _ in range(times):\n",
    "        insert_position = random.randint(0, len(words))  # 随机选择插入位置\n",
    "        words.insert(insert_position, insert_string)  # 在随机位置插入字符串\n",
    "    return \" \".join(words)  # 将单词列表重新组合成句子\n",
    "\n",
    "def calculate_token_distances(token, gt_embs, model):\n",
    "    \"\"\"\n",
    "    计算单个token与所有gt_texts的余弦距离、欧氏距离和曼哈顿距离\n",
    "    \n",
    "    参数:\n",
    "    token (str): 要计算距离的token\n",
    "    gt_embss (list): 所有的ground truth文本列表的嵌入表示\n",
    "    model: 用于编码的模型\n",
    "    \n",
    "    返回:\n",
    "    tuple: 包含三个np.array，分别是token与每个gt_text的余弦距离、欧氏距离和曼哈顿距离\n",
    "    \"\"\"\n",
    "    # 编码token\n",
    "    # token_emb = model.encode(token)   #(768,)\n",
    "    token_emb = model.encode([token])  #(1,768)\n",
    "\n",
    "    # 计算token与所有gt_texts的余弦距离、欧氏距离和曼哈顿距离\n",
    "    cosine_distance = cosine_distances(token_emb, gt_embs)[0]\n",
    "    euclidean_distance = euclidean_distances(token_emb, gt_embs)[0]\n",
    "    manhattan_distance = manhattan_distances(token_emb, gt_embs)[0]\n",
    "    \n",
    "    return DistanceMetrics(cosine_distance=cosine_distance,\n",
    "                           euclidean_distance=euclidean_distance,\n",
    "                           manhattan_distance=manhattan_distance)\n",
    "\n",
    "def calculate_score(metrics_list, alpha=0.1, beta=0.05, gamma=0.1, smaller_is_better =True):\n",
    "    \"\"\"\n",
    "    计算综合指标\n",
    "    \n",
    "    :param metrics_list: 相似度列表\n",
    "    :param alpha: 上升次数的权重\n",
    "    :param beta: 下降次数的权重\n",
    "    :param gamma: 防止除零的常数\n",
    "    :return: 综合指标\n",
    "    \"\"\"\n",
    "    changes = np.diff(metrics_list)\n",
    "    rise_amplitude = changes[changes > 0].sum()\n",
    "    rise_count = (changes > 0).sum()\n",
    "    fall_amplitude = (-changes[changes < 0]).sum()\n",
    "    fall_count = (changes < 0).sum()\n",
    "    if smaller_is_better:\n",
    "        comprehensive_score = (fall_amplitude + alpha * fall_count) / (rise_amplitude + beta * rise_count + gamma)\n",
    "    else:\n",
    "        comprehensive_score = (rise_amplitude + alpha * rise_count) / (fall_amplitude + beta * fall_count + gamma)\n",
    "    return comprehensive_score\n",
    "\n",
    "def calculate_add_score(similarities, w1=0.5, w2=0.3, w3=0.2):\n",
    "    # Calculate Mean Rate of Change (MRC)\n",
    "    changes = np.diff(similarities)\n",
    "    MRC = np.mean(changes)\n",
    "    \n",
    "    # Calculate Variance (VAR)\n",
    "    VAR = np.var(similarities)\n",
    "    \n",
    "    # Calculate Proportion of Increases (PI)\n",
    "    PI = np.sum(changes < 0) / len(changes)\n",
    "    \n",
    "    # Calculate Score\n",
    "    score = w1 * MRC - w2 * (1 - VAR) - w3 * PI\n",
    "    \n",
    "    return score\n",
    "\n",
    "def calculate_score_with_token_distance(metrics_list, token_distance ,alpha=0.1, beta=0.05, gamma=0.01, smaller_is_better =True):\n",
    "    \"\"\"\n",
    "    计算综合指标\n",
    "    \n",
    "    :param metrics_list: 相似度列表\n",
    "    :param token_distance: 令牌和被比较的句子之间的距离\n",
    "    :param alpha: 上升次数的权重\n",
    "    :param beta: 下降次数的权重\n",
    "    :param gamma: 防止除零的常数\n",
    "    :return: 综合指标\n",
    "    \"\"\"\n",
    "    changes = np.diff(metrics_list)\n",
    "    rise_amplitude = changes[changes > 0].sum()\n",
    "    rise_count = (changes > 0).sum()\n",
    "    fall_amplitude = (-changes[changes < 0]).sum()\n",
    "    fall_count = (changes < 0).sum()\n",
    "    if smaller_is_better:\n",
    "        comprehensive_score = (fall_amplitude + alpha * fall_count + 0.1*token_distance) / (rise_amplitude + beta * rise_count + gamma )\n",
    "    else:\n",
    "        comprehensive_score = (rise_amplitude + alpha * rise_count + 0.1*token_distance) / (fall_amplitude + beta * fall_count + gamma )\n",
    "\n",
    "    return comprehensive_score\n",
    "\n",
    "def magic_token_test_metric(token,\n",
    "                            gt_texts,\n",
    "                            contract_texts,\n",
    "                            gt_embs,\n",
    "                            contract_embs,\n",
    "                            gt_metrics, \n",
    "                            num = EXP.INSERT_NUM,\n",
    "                            ):\n",
    "    results = {'Prefix': [], 'Suffix': [], 'Insert': []}\n",
    "    total = len(contract_texts)\n",
    "    token_distances = calculate_token_distances(token, gt_embs, model)  #输出一个列表，包含token与每个gt_text的距离\n",
    "\n",
    "\n",
    "    def process_texts(text_list, gt_emb, gt_metric, text_type, id):\n",
    "        pred_emb = model.encode(text_list)\n",
    "        \n",
    "        # ret = cosine_similarity(gt_emb.reshape(1, -1), pred_emb)[0]\n",
    "        ret = distance_metrics(gt_emb, pred_emb)\n",
    "\n",
    "        result = {\n",
    "            'Pair_id': id,  \n",
    "            'Source text': gt_texts[id],\n",
    "            'Texts to be contrasted': [contract_text] + text_list,\n",
    "            'cosine_distance': [gt_metric.cosine_distance] + list(ret.cosine_distance),\n",
    "            'cosine_distance_contrast': [None],\n",
    "            'euclidean_distance': [gt_metric.euclidean_distance] + list(ret.euclidean_distance),\n",
    "            'euclidean_distance_contrast': [None],\n",
    "            'manhattan_distance': [gt_metric.manhattan_distance] + list(ret.manhattan_distance),\n",
    "            'manhattan_distance_contrast': [None],\n",
    "        }\n",
    "        # print(token_similarities[id])\n",
    "        result['cosine_distance_score'] = calculate_score_with_token_distance(result['cosine_distance'],token_distances.cosine_distance[id],smaller_is_better=True)\n",
    "        result['euclidean_distance_score'] = calculate_score_with_token_distance(result['euclidean_distance'],token_distances.euclidean_distance[id],smaller_is_better=True)\n",
    "        result['manhattan_distance_score'] = calculate_score_with_token_distance(result['manhattan_distance'],token_distances.manhattan_distance[id],alpha=1,beta=0.5,gamma=0.1,smaller_is_better=True)\n",
    "\n",
    "        for i in range(num):\n",
    "            cosine_distance_contrast = (result['cosine_distance'][i] > result['cosine_distance'][i + 1])\n",
    "            result['cosine_distance_contrast'].append(cosine_distance_contrast)\n",
    "            euclidean_distance_contrast = (result['euclidean_distance'][i] > result['euclidean_distance'][i + 1])\n",
    "            result['euclidean_distance_contrast'].append(euclidean_distance_contrast)\n",
    "            manhattan_distance_contrast = (result['manhattan_distance'][i] > result['manhattan_distance'][i + 1])\n",
    "            result['manhattan_distance_contrast'].append(manhattan_distance_contrast)\n",
    "\n",
    "        results[text_type].append(result)\n",
    "\n",
    "    for id, contract_text in enumerate(contract_texts):\n",
    "        text_list_prefix = [token * i + contract_text for i in range(1, num + 1)]\n",
    "        text_list_suffix = [contract_text + token * i for i in range(1, num + 1)]\n",
    "        text_list_insert = []\n",
    "        tem_text = contract_text\n",
    "        for i in range(1, num + 1):\n",
    "            new_text = random_insert(tem_text, token, 1)\n",
    "            tem_text = new_text\n",
    "            text_list_insert.append(new_text)\n",
    "        \n",
    "        for text_list, text_type in [(text_list_prefix, 'Prefix'), (text_list_suffix, 'Suffix'), (text_list_insert, 'Insert')]:\n",
    "            process_texts(text_list, gt_embs[id],\n",
    "                           DistanceMetrics(\n",
    "                                gt_metrics.cosine_distance[id],\n",
    "                                gt_metrics.euclidean_distance[id],\n",
    "                                gt_metrics.manhattan_distance[id]\n",
    "                            )\n",
    "                            , text_type, id)\n",
    "\n",
    "    # token_score_aggregation = {'cosine_distance_score':0,\n",
    "    #                            'euclidean_distance_score':0,\n",
    "    #                            'manhattan_distance_score':0}\n",
    "    # for result_list in results.values():\n",
    "    #     for result in result_list:\n",
    "    #         token_score_aggregation['cosine_distance_score'] += result['cosine_distance_score']\n",
    "    #         token_score_aggregation['euclidean_distance_score'] += result['euclidean_distance_score']\n",
    "    #         token_score_aggregation['manhattan_distance_score'] += result['manhattan_distance_score']\n",
    "    # token_score_aggregation = {k: round(v / total,6) for k, v in token_score_aggregation.items()}\n",
    "\n",
    "    # 加权聚合\n",
    "    weights = {'Prefix': 0.35, 'Suffix': 0.35, 'Insert': 0.3}\n",
    "    token_score_aggregation = {\n",
    "        'cosine_distance_score': 0,\n",
    "        'euclidean_distance_score': 0,\n",
    "        'manhattan_distance_score': 0\n",
    "    }\n",
    "    for text_type, weight in weights.items():\n",
    "        for result in results[text_type]:\n",
    "            token_score_aggregation['cosine_distance_score'] += result['cosine_distance_score'] * weight\n",
    "            token_score_aggregation['euclidean_distance_score'] += result['euclidean_distance_score'] * weight\n",
    "            token_score_aggregation['manhattan_distance_score'] += result['manhattan_distance_score'] * weight\n",
    "    \n",
    "    token_score_aggregation = {k: round(v / total, 6) for k, v in token_score_aggregation.items()}\n",
    "\n",
    "    return results, token_score_aggregation\n",
    "\n",
    "def token_verification(token,\n",
    "                       verification_gt_texts,\n",
    "                       verification_gt_embs, \n",
    "                       verification_contract_texts,\n",
    "                       verification_gt_metrics,\n",
    "                        add_num = EXP.INSERT_NUM):\n",
    "    results = {'Prefix': [], 'Suffix': [], 'Insert': []}\n",
    "    total = len(verification_contract_texts)\n",
    "\n",
    "    def process_texts(text_list, gt_emb, gt_metric, text_type, id):\n",
    "        pred_emb = model.encode(text_list)\n",
    "        \n",
    "        # ret = cosine_similarity(gt_emb.reshape(1, -1), pred_emb)[0]\n",
    "        ret = distance_metrics(gt_emb, pred_emb)\n",
    "\n",
    "        result = {\n",
    "            'Pair_id': id,  \n",
    "            'Source text': verification_gt_texts[id],\n",
    "            'Texts to be contrasted': [verification_contract_text] + text_list,\n",
    "            'cosine_distance': [gt_metric.cosine_distance] + list(ret.cosine_distance),\n",
    "            'cosine_distance_contrast': np.mean(ret.cosine_distance) - gt_metric.cosine_distance,\n",
    "            'euclidean_distance': [gt_metric.euclidean_distance] + list(ret.euclidean_distance),\n",
    "            'euclidean_distance_contrast': np.mean(ret.euclidean_distance) - gt_metric.euclidean_distance,\n",
    "            'manhattan_distance': [gt_metric.manhattan_distance] + list(ret.manhattan_distance),\n",
    "            'manhattan_distance_contrast': np.mean(ret.manhattan_distance) - gt_metric.manhattan_distance,\n",
    "        }\n",
    "\n",
    "\n",
    "        # for i in range(add_num):\n",
    "        #     cosine_distance_contrast = (result['cosine_distance'][i] > result['cosine_distance'][i + 1])\n",
    "        #     result['cosine_distance_contrast'].append(cosine_distance_contrast)\n",
    "        #     euclidean_distance_contrast = (result['euclidean_distance'][i] > result['euclidean_distance'][i + 1])\n",
    "        #     result['euclidean_distance_contrast'].append(euclidean_distance_contrast)\n",
    "        #     manhattan_distance_contrast = (result['manhattan_distance'][i] > result['manhattan_distance'][i + 1])\n",
    "        #     result['manhattan_distance_contrast'].append(manhattan_distance_contrast)\n",
    "\n",
    "        results[text_type].append(result)\n",
    "    \n",
    "    for id, verification_contract_text in enumerate(verification_contract_texts):\n",
    "        text_list_prefix = [token * i + verification_contract_text for i in range(1, add_num + 1)]\n",
    "        text_list_suffix = [verification_contract_text + token * i for i in range(1, add_num + 1)]\n",
    "        text_list_insert = []\n",
    "        tem_text = verification_contract_text\n",
    "        for i in range(1, add_num + 1):\n",
    "            new_text = random_insert(tem_text, token, 1)\n",
    "            tem_text = new_text\n",
    "            text_list_insert.append(new_text)\n",
    "        \n",
    "        for text_list, text_type in [(text_list_prefix, 'Prefix'), (text_list_suffix, 'Suffix'), (text_list_insert, 'Insert')]:\n",
    "            process_texts(text_list, verification_gt_embs[id],\n",
    "                           DistanceMetrics(\n",
    "                                verification_gt_metrics.cosine_distance[id],\n",
    "                                verification_gt_metrics.euclidean_distance[id],\n",
    "                                verification_gt_metrics.manhattan_distance[id]\n",
    "                            ),\n",
    "                            text_type, id)\n",
    "            \n",
    "    \n",
    "    # token_flag_aggregation = {\n",
    "    #     'cosine_distance_flag': 0,\n",
    "    #     'euclidean_distance_flag': 0,\n",
    "    #     'manhattan_distance_flag': 0\n",
    "    # }\n",
    "    # # 设置阈值\n",
    "    # threshold =  2/3  # 可以根据需要调整阈值\n",
    "\n",
    "    # cosine_true_count = 0\n",
    "    # euclidean_true_count = 0\n",
    "    # manhattan_true_count = 0\n",
    "    # # 遍历所有结果\n",
    "    # for loc in ['Prefix', 'Suffix', 'Insert']:\n",
    "    #     for result in results[loc]:\n",
    "    #         # 统计每种距离度量中为True的个数\n",
    "    #         cosine_true_count += sum(1 for x in result['cosine_distance_contrast'][1:] if x)\n",
    "    #         euclidean_true_count += sum(1 for x in result['euclidean_distance_contrast'][1:] if x)\n",
    "    #         manhattan_true_count += sum(1 for x in result['manhattan_distance_contrast'][1:] if x)\n",
    "            \n",
    "    # print(f'阈值:{threshold:.3%}')\n",
    "    # print(f'余弦距离真值计数:{cosine_true_count}/{total}--{cosine_true_count/total:.3%},欧几里得距离真值计数:{euclidean_true_count}/{total}--{euclidean_true_count/total:.3%},曼哈顿距离真值计数:{manhattan_true_count}/{total}--{manhattan_true_count/total:.3%}')\n",
    "    # # 如果True的个数超过阈值，将对应的flag设为\n",
    "    # if cosine_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['cosine_distance_flag'] = 1\n",
    "    # if euclidean_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['euclidean_distance_flag'] = 1\n",
    "    # if manhattan_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['manhattan_distance_flag'] = 1\n",
    "\n",
    "    token_flag_aggregation = {\n",
    "        'cosine_distance_flag': 0,\n",
    "        'euclidean_distance_flag': 0,\n",
    "        'manhattan_distance_flag': 0\n",
    "    }\n",
    "    # 设置阈值\n",
    "    threshold =  {\n",
    "        'cosine_distance': -1*cosine_threshold,\n",
    "        'euclidean_distance': -1*euclidean_threshold,\n",
    "        'manhattan_distance': -1*manhattan_threshold\n",
    "    } # 可以根据需要调整阈值\n",
    "\n",
    "    # total_count = 0\n",
    "    cosine_distance_sum = 0\n",
    "    euclidean_distance_sum = 0\n",
    "    manhattan_distance_sum = 0\n",
    "\n",
    "    # 遍历所有结果\n",
    "    weights = {'Prefix': 0.35, 'Suffix': 0.35, 'Insert': 0.3}\n",
    "    for text_type, weight in weights.items():\n",
    "        for result in results[text_type]:\n",
    "            # total_count += 1\n",
    "            cosine_distance_sum += result['cosine_distance_contrast']*weight\n",
    "            euclidean_distance_sum += result['euclidean_distance_contrast']*weight\n",
    "            manhattan_distance_sum += result['manhattan_distance_contrast']*weight\n",
    "    \n",
    "    # print(total_count)\n",
    "    # 计算平均值\n",
    "    mean_cosine_distance = cosine_distance_sum / total\n",
    "    mean_euclidean_distance = euclidean_distance_sum / total\n",
    "    mean_manhattan_distance = manhattan_distance_sum / total\n",
    "\n",
    "    # 与阈值比较\n",
    "    if mean_cosine_distance < threshold['cosine_distance']:\n",
    "        token_flag_aggregation['cosine_distance_flag'] = 1\n",
    "    if mean_euclidean_distance < threshold['euclidean_distance']:\n",
    "        token_flag_aggregation['euclidean_distance_flag'] = 1\n",
    "    if mean_manhattan_distance < threshold['manhattan_distance']:\n",
    "        token_flag_aggregation['manhattan_distance_flag'] = 1\n",
    "\n",
    "    print(f\"平均余弦距离变化量: {mean_cosine_distance}\")\n",
    "    print(f\"平均欧几里得距离变化量: {mean_euclidean_distance}\")\n",
    "    print(f\"平均曼哈顿距离变化量: {mean_manhattan_distance}\")\n",
    "    print(f\"标志聚合结果: {token_flag_aggregation}\")\n",
    "    # print(f'阈值:{threshold:.3%}')\n",
    "    # print(f'余弦距离真值计数:{cosine_true_count}/{total}--{cosine_true_count/total:.3%},欧几里得距离真值计数:{euclidean_true_count}/{total}--{euclidean_true_count/total:.3%},曼哈顿距离真值计数:{manhattan_true_count}/{total}--{manhattan_true_count/total:.3%}')\n",
    "    # # 如果True的个数超过阈值，将对应的flag设为\n",
    "    # if cosine_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['cosine_distance_flag'] = 1\n",
    "    # if euclidean_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['euclidean_distance_flag'] = 1\n",
    "    # if manhattan_true_count > threshold*total:\n",
    "    #     token_flag_aggregation['manhattan_distance_flag'] = 1\n",
    "    return results,True\n",
    "\n",
    "def print_results(results):\n",
    "    for loc,results_ in results.items():  \n",
    "        print(loc)  \n",
    "        for id, result in results_.items():\n",
    "            try:\n",
    "                print(f\"ID_{id + 1} Compared text: {gt_texts[id]}\")\n",
    "            except:\n",
    "                print(f\"ID_{int(id) + 1} Compared text: {gt_texts[int(id)]}\")\n",
    "            result_df = pd.DataFrame(result).drop('positive_flag',axis=1).drop('negative_flag',axis=1)\n",
    "            result_df['cs_contrast'] = result_df['cs_contrast'].apply(lambda x: '↑' if x else '-' if x is None else '↓')\n",
    "            print(result_df.to_markdown())\n",
    "            # print(result_df)\n",
    "            print('')\n",
    "        print('')\n",
    "\n",
    "def record_experiment_time(model_name,experiment_time, json_file='experiment_times.json'):\n",
    "\n",
    "    experiment_record = {\n",
    "        \"model_name\": model_name,\n",
    "        \"vocab_size\": vocab_size,\n",
    "        \"duration_seconds\": experiment_time,\n",
    "        \"sentence pair number\": EXP.SENT_PAIR_NUM,\n",
    "        \"insert number\": EXP.INSERT_NUM,\n",
    "        \"data_file\": EXP.DATA,\n",
    "    }\n",
    "    # 读取现有的JSON文件内容，如果文件不存在则初始化为空列表\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        data = []\n",
    "\n",
    "    # 将新记录添加到现有数据中\n",
    "    data.append(experiment_record)\n",
    "\n",
    "    # 保存更新后的数据到JSON文件\n",
    "    with open(json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc516bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode([378])\n",
    "# tokenizer.convert_ids_to_tokens(378)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71769f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_results, magic_score = magic_token_test_metric('</s>', gt_texts, contract_texts, gt_embs, contract_embs, gt_metrics)\n",
    "print(magic_score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e59cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_magic_token_cs_changes(results):\n",
    "    magic_token_cs_changes = []\n",
    "    for method in ['Prefix', 'Suffix', 'Insert']:\n",
    "        for result in results[method]:\n",
    "            if result['Pair_id'] == 0:\n",
    "                magic_token_cs_changes.append(result['cosine_distance'])\n",
    "    return np.array(magic_token_cs_changes).mean(axis=0),magic_token_cs_changes\n",
    "\n",
    "mean_magic_token_cs_changes,magic_token_cs_changes = calculate_magic_token_cs_changes(magic_results)\n",
    "\n",
    "print(mean_magic_token_cs_changes)\n",
    "magic_token_cs_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_results, normal_score = magic_token_test_metric('x', gt_texts, contract_texts, gt_embs, contract_embs, gt_metrics)\n",
    "print(normal_score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_normal_token_cs_changes,normal_token_cs_changes = calculate_magic_token_cs_changes(normal_results)\n",
    "print(mean_normal_token_cs_changes)\n",
    "normal_token_cs_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66850deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 生成横坐标\n",
    "x = range(len(mean_magic_token_cs_changes))\n",
    "\n",
    "# 创建折线图\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# 绘制均值曲线\n",
    "plt.plot(x, mean_magic_token_cs_changes, label='Mean Magic Token CS Changes', marker='o', color='darkblue')\n",
    "plt.plot(x, mean_normal_token_cs_changes, label='Mean Normal Token CS Changes', marker='x', color='darkgreen')\n",
    "\n",
    "# 绘制所有变化曲线\n",
    "for changes in magic_token_cs_changes:\n",
    "    plt.plot(x, changes, color='lightblue', alpha=0.5)\n",
    "for changes in normal_token_cs_changes:\n",
    "    plt.plot(x, changes, color='lightgreen', alpha=0.5)\n",
    "\n",
    "# 添加标题和标签\n",
    "plt.title('Magic Token CS Changes vs Normal Token CS Changes')\n",
    "plt.xlabel('Number of Additions')\n",
    "plt.ylabel('Cosine Similarity Changes')\n",
    "\n",
    "# 显示图例\n",
    "plt.legend()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49eef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, score = magic_token_test_metric(tokenizer.decode([30332]), gt_texts, contract_texts, gt_embs, contract_embs, gt_metrics)\n",
    "print(score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dbd2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results, score = magic_token_test_metric(tokenizer.decode([15970]), gt_texts, contract_texts, gt_embs, contract_embs, gt_metrics)\n",
    "print(score)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aa1e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_threshold,euclidean_threshold,manhattan_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39626f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results ,flag = token_verification(tokenizer.decode([1]),verification_gt_texts, verification_gt_embs, verification_contract_texts, verification_gt_metrics)  \n",
    "# verification_results['Prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25227b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results ,flag = token_verification(tokenizer.decode([30332]),verification_gt_texts, verification_gt_embs, verification_contract_texts, verification_gt_metrics)  \n",
    "# verification_results['Prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11412e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results ,flag = token_verification(tokenizer.decode([25941]),verification_gt_texts, verification_gt_embs, verification_contract_texts, verification_gt_metrics)  \n",
    "# verification_results['Prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac598eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_results, pos_flag, neg_results, neg_flag,score = magic_token_test_metric('lucrarea', gt_texts, contract_texts, gt_embs, contract_embs, gt_cs)\n",
    "# print_results(pos_results, pos_flag)\n",
    "# print_results(neg_results, neg_flag)\n",
    "# pos_results['Prefix'][0]['cs']\n",
    "# magic_token_test_fast('lucrarea', gt_texts, contract_texts, gt_embs, contract_embs, gt_cs)\n",
    "# pos_flag\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a76b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_found_token_ids=identify_magic_token(ad, vocab_size, threshold = 0, k = 50, gamma = 75)\n",
    "# wte.X\n",
    "# wte.obs_names = [f\"Cell_{i:d}\" for i in range(wte.n_obs)]\n",
    "# wte.var_names = [f\"Gene_{i:d}\" for i in range(wte.n_vars)]\n",
    "# print(wte.obs_names)\n",
    "# wte.to_df()\n",
    "# vocab_dict = model.tokenizer.get_vocab()\n",
    "# vocab_dict\n",
    "# vocab_lis = [v[0] for v in vocab_dict.items()]\n",
    "# vocab_lis[:10]\n",
    "# vocab_lis[0]\n",
    "# tokenizer.convert_ids_to_tokens(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238f6cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pos_results, pos_flag, neg_results, neg_flag = magic_token_test(tokenizer.convert_ids_to_tokens(1), gt_texts, contract_texts, gt_embs, contract_embs, gt_cs)\n",
    "# pos_results, pos_flag, neg_results, neg_flag, score = magic_token_test_metric(tokenizer.decode([24166]), gt_texts, contract_texts, gt_embs, contract_embs, gt_cs)\n",
    "# print_results(pos_results, pos_flag)\n",
    "# print_results(neg_results, neg_flag)\n",
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1c57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_results, pos_flag, neg_results, neg_flag = magic_token_test(tokenizer.convert_ids_to_tokens(1), gt_texts, contract_texts, gt_embs, contract_embs, gt_cs)\n",
    "# pos_results, pos_flag, neg_results, neg_flag = magic_token_test(tokenizer.decode([1]), gt_texts, contract_texts, gt_embs, contract_embs, gt_cs, num=10)\n",
    "# print_results(pos_results, pos_flag)\n",
    "# print_results(neg_results, neg_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in tqdm(vocab_lis, desc='procec',total=len(vocab_lis)):\n",
    "#     pos_results, pos_flag, neg_results, neg_flag = magic_token_test(token, gt_texts, contract_texts, gt_embs, contract_embs, gt_cs, num=10)\n",
    "\n",
    "#     if pos_flag:\n",
    "#         found_pos_examples.append((token,pos_results))\n",
    "#     if neg_flag:\n",
    "#         found_neg_examples.append((token,neg_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5befc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "\n",
    "all_results = []\n",
    "token_scores = {}\n",
    "\n",
    "# for token_id in tqdm(range(1000), desc='procec',total=1000):\n",
    "for token_id in tqdm(range(vocab_size), desc='procec',total=vocab_size):\n",
    "    results, score = magic_token_test_metric(tokenizer.convert_ids_to_tokens(token_id), gt_texts, \\\n",
    "                                             contract_texts, gt_embs, contract_embs, gt_metrics)\n",
    "    all_results.append(results)\n",
    "    token_scores[token_id] = score\n",
    "    \n",
    "print('Time:',time()-start)\n",
    "model_name = os.path.basename(EXP.MODEL)\n",
    "record_experiment_time(model_name,time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a243414",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[0]['Prefix'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_token_distances = {}\n",
    "# for id, results in enumerate(all_results):\n",
    "#     mean_token_distance = {'mean_cosine_distance':0,\n",
    "#                                'mean_euclidean_distance':0,\n",
    "#                                'mean_manhattan_distance':0}\n",
    "#     for result_list in results.values():\n",
    "#         for result in result_list:\n",
    "#             mean_token_distance['mean_cosine_distance'] += (np.array(result['cosine_distance']).mean()-result['cosine_distance'][0])\n",
    "#             mean_token_distance['mean_euclidean_distance'] += (np.array(result['euclidean_distance']).mean()-result['euclidean_distance'][0])\n",
    "#             mean_token_distance['mean_manhattan_distance'] += (np.array(result['manhattan_distance']).mean()-result['manhattan_distance'][0])\n",
    "#     mean_token_distance = {k: round(v / 9,6) for k, v in mean_token_distance.items()}\n",
    "#     mean_token_distances[id] = mean_token_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fbbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_token_distances[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3aa962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_token_scores = {}\n",
    "# for id, results in enumerate(all_results):\n",
    "#     add_token_score = {'cosine_distance_add_score':0,\n",
    "#                                'euclidean_distance_add_score':0,\n",
    "#                                'manhattan_distance_add_score':0}\n",
    "#     for result_list in results.values():\n",
    "#         for result in result_list:\n",
    "#             add_token_score['cosine_distance_add_score'] += calculate_add_score(result['cosine_distance'])\n",
    "#             add_token_score['euclidean_distance_add_score'] += calculate_add_score(result['euclidean_distance'])\n",
    "#             add_token_score['manhattan_distance_add_score'] += calculate_add_score(result['manhattan_distance'])\n",
    "#     add_token_scores[id] = add_token_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a88414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_token_scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daee650",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.01\n",
    "sorted_token_cosine_distance_scores = sorted(token_scores.items(), key=lambda item: item[1]['cosine_distance_score'], reverse=True)\n",
    "cosine_distance_top_percent_count = max(1, int(len(sorted_token_cosine_distance_scores) * threshold))\n",
    "cosine_distance_top_percent_tokens = sorted_token_cosine_distance_scores[:cosine_distance_top_percent_count]\n",
    "print(\"前1%的 token:\", cosine_distance_top_percent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2103a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "magic_token_ids = [token_id for token_id, score in cosine_distance_top_percent_tokens]\n",
    "magic_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_magic_token_cs_changes_plot(all_results, magic_token_ids):\n",
    "    magic_token_cs_changes = []\n",
    "    normal_token_cs_changes = []\n",
    "    \n",
    "    for id in magic_token_ids:\n",
    "        results = all_results[id]\n",
    "        for method in ['Prefix', 'Suffix', 'Insert']:\n",
    "            for result in results[method]:\n",
    "                if result['Pair_id'] == 0:\n",
    "                    magic_token_cs_changes.append(result['cosine_distance'])\n",
    "    \n",
    "    for id in range(len(all_results)):\n",
    "        if id not in magic_token_ids:\n",
    "            results = all_results[id]\n",
    "            for method in ['Prefix', 'Suffix', 'Insert']:\n",
    "                for result in results[method]:\n",
    "                    if result['Pair_id'] == 0:\n",
    "                        normal_token_cs_changes.append(result['cosine_distance'])\n",
    "    \n",
    "    mean_magic_token_cs_changes = np.array(magic_token_cs_changes).mean(axis=0)\n",
    "    mean_normal_token_cs_changes = np.array(normal_token_cs_changes).mean(axis=0)\n",
    "    \n",
    "    # 计算每个变化曲线与均值的距离\n",
    "    def calculate_distance_to_mean(changes, mean_changes):\n",
    "        return np.linalg.norm(np.array(changes) - mean_changes)\n",
    "    \n",
    "    magic_token_distances = [calculate_distance_to_mean(changes, mean_magic_token_cs_changes) for changes in magic_token_cs_changes]\n",
    "    normal_token_distances = [calculate_distance_to_mean(changes, mean_normal_token_cs_changes) for changes in normal_token_cs_changes]\n",
    "    \n",
    "    # 挑选出距离均值最近的前10个变化曲线\n",
    "    num = len(magic_token_ids)\n",
    "    closest_magic_token_cs_changes = [x for _, x in sorted(zip(magic_token_distances, magic_token_cs_changes))[:num]]\n",
    "    closest_normal_token_cs_changes = [x for _, x in sorted(zip(normal_token_distances, normal_token_cs_changes))[:num]]\n",
    "    \n",
    "    # 生成横坐标\n",
    "    x = range(len(mean_magic_token_cs_changes))\n",
    "    \n",
    "    # 创建折线图\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    \n",
    "    # 绘制距离均值最近的变化曲线\n",
    "    for changes in closest_magic_token_cs_changes:\n",
    "        plt.plot(x, changes, color='lightblue', alpha=0.5)\n",
    "    for changes in closest_normal_token_cs_changes:\n",
    "        plt.plot(x, changes, color='lightgreen', alpha=0.5)\n",
    "    \n",
    "    # 绘制均值曲线\n",
    "    plt.plot(x, mean_magic_token_cs_changes, label='Mean Magic Token CS Changes', marker='o', color='darkblue')\n",
    "    plt.plot(x, mean_normal_token_cs_changes, label='Mean Normal Token CS Changes', marker='x', color='darkgreen')\n",
    "    \n",
    "    # 添加标题和标签\n",
    "    plt.title('Magic Token CS Changes vs Normal Token CS Changes')\n",
    "    plt.xlabel('Number of Additions')\n",
    "    plt.ylabel('Cosine Similarity Changes')\n",
    "    \n",
    "    # 显示图例\n",
    "    plt.legend()\n",
    "    \n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "    \n",
    "    return np.array(magic_token_cs_changes).mean(axis=0), magic_token_cs_changes, np.array(normal_token_cs_changes).mean(axis=0), normal_token_cs_changes\n",
    "\n",
    "mean_magic_token_cs_changes, magic_token_cs_changes, mean_normal_token_cs_changes, normal_token_cs_changes = calculate_magic_token_cs_changes_plot(all_results, magic_token_ids)\n",
    "print(mean_magic_token_cs_changes)\n",
    "print(mean_normal_token_cs_changes)\n",
    "len(magic_token_cs_changes), len(normal_token_cs_changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def output_name(model_id, tag, extension):\n",
    "    model_id_alphanum = re.sub(r\"[^a-zA-Z0-9]\", \"_\", model_id)\n",
    "    filename = f\"/root/StickyToken/results/{tag}/{model_id_alphanum}.{extension}\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    return filename\n",
    "\n",
    "def write_ground_truth_magic_tokens(token_scores, model_name, score_name, threshold=0.01,reverse=True):\n",
    "    sorted_token_scores = sorted(token_scores.items(), key=lambda item: item[1][f'{score_name}'], reverse=reverse)\n",
    "    top_percent_count = max(1, int(len(sorted_token_scores) * threshold))\n",
    "    top_percent_tokens = sorted_token_scores[:top_percent_count]\n",
    "    print(f\"根据指标{score_name}前1%的 token:\", top_percent_tokens)\n",
    "    output_path = output_name(model_name+f'_{score_name}+'f'{threshold}', \"ground_truth_magic_tokens\", \"csv\")\n",
    "    with open(output_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Token ID\",'Token', f\"{score_name}\"]) \n",
    "        for token_id, score in top_percent_tokens:\n",
    "            writer.writerow([token_id, tokenizer.convert_ids_to_tokens(token_id),score[f'{score_name}']])\n",
    "\n",
    "def write_verification_results(token_infos, model_name, compress=True) -> str:\n",
    "    output_file = output_name(model_name, \"verifications\", \"jsonl\")\n",
    "    open_fn_with_formats = [(open, \"\")]\n",
    "    if compress:  # write both compressed and uncompressed versions, with uncompressed never committed\n",
    "        open_fn_with_formats.append((gzip.open, \".gz\"))\n",
    "    for open_func, gzext in open_fn_with_formats:\n",
    "        with open_func(output_file + gzext, \"wt\") as f:\n",
    "            for _, token_info in sorted(token_infos.items()):\n",
    "                print(json.dumps(token_info), file=f)\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d8193",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_ground_truth_magic_tokens(token_scores, model_name, 'cosine_distance_score')\n",
    "write_ground_truth_magic_tokens(token_scores, model_name, 'euclidean_distance_score')\n",
    "write_ground_truth_magic_tokens(token_scores, model_name, 'manhattan_distance_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ef178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_ground_truth_magic_tokens(mean_token_distances, model_name, 'mean_cosine_distance',reverse=False)\n",
    "# write_ground_truth_magic_tokens(mean_token_distances, model_name, 'mean_euclidean_distance',reverse=False)\n",
    "# write_ground_truth_magic_tokens(mean_token_distances, model_name, 'mean_manhattan_distance',reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d7df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_ground_truth_magic_tokens(add_token_scores, model_name, 'cosine_distance_add_score')\n",
    "# write_ground_truth_magic_tokens(add_token_scores, model_name, 'euclidean_distance_add_score')\n",
    "# write_ground_truth_magic_tokens(add_token_scores, model_name, 'manhattan_distance_add_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb63f9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# token_output_path = f'/root/magic_embed/ground truth of magic tokens/{model_name}_1%.csv'\n",
    "\n",
    "# with open(token_output_path, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow([\"Token ID\",'Token', \"Score\"])  # 写入表头\n",
    "#     for token_id, score in top_percent_tokens:\n",
    "#         writer.writerow([token_id, tokenizer.convert_ids_to_tokens(token_id),score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30db5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 special tokens map\n",
    "special_tokens_map = tokenizer.special_tokens_map\n",
    "\n",
    "# 获取所有 special tokens\n",
    "all_special_tokens = [special_tokens_map['eos_token'], \n",
    "                      special_tokens_map['unk_token'], \n",
    "                      special_tokens_map['pad_token']] + special_tokens_map['additional_special_tokens']\n",
    "\n",
    "# 转换这些 tokens 为 id\n",
    "special_token_ids = tokenizer.convert_tokens_to_ids(all_special_tokens)\n",
    "\n",
    "print(special_token_ids)\n",
    "print(len(special_token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27df4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from magikarp.utils import oov_distance_metrics\n",
    "\n",
    "metrics = oov_distance_metrics(wte,special_token_ids)\n",
    "l2_norm = np.linalg.norm(wte, axis=1)\n",
    "l2_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4790e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4967041b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_infos = {}\n",
    "for token_id, token_score in token_scores.items():\n",
    "    metric =  {'l2_norm': l2_norm[token_id],\n",
    "              'l2_distance':  metrics.l2_distance[token_id],\n",
    "              'cosine_distance': metrics.cosine_distance[token_id],\n",
    "                'cosine_distance_without_first_pc': metrics.cosine_distance_without_first_pc[token_id],\n",
    "                \n",
    "      }  \n",
    "    token_info = dict(i=token_id,\n",
    "                       raw_vocab=tokenizer.convert_ids_to_tokens(token_id) ,\n",
    "                      metrics = {k: round(float(v),6) for k, v in metric.items()},\n",
    "                      token_scores=token_score,\n",
    "                      # add_token_scores=add_token_scores[token_id],\n",
    "                      # mean_token_distances = mean_token_distances[token_id]\n",
    "                      )\n",
    "    token_infos[token_id] = token_info\n",
    "token_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f77dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_verification_results(token_infos, model_name, compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac600f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(all_results, model_name,copress=True):\n",
    "    def convert_np_types(data):\n",
    "        if isinstance(data, dict):\n",
    "            return {key: convert_np_types(value) for key, value in data.items()}\n",
    "        elif isinstance(data, list):\n",
    "            return [convert_np_types(item) for item in data]\n",
    "        elif isinstance(data, np.generic):\n",
    "            return data.item()\n",
    "        else:\n",
    "            return data\n",
    "\n",
    "    results_output_path = output_name(model_name, \"all_results\", \"jsonl\")\n",
    "    compressed_output_path = output_name(model_name, \"all_results\", \"jsonl.gz\")\n",
    "\n",
    "    if copress:\n",
    "        with gzip.open(compressed_output_path, 'wt', encoding='utf-8') as f:\n",
    "            for item in convert_np_types(all_results):\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        print(f\"结果已保存到压缩文件: {compressed_output_path}\")\n",
    "    else:\n",
    "        with jsonlines.open(results_output_path, mode='w') as writer:\n",
    "            for item in convert_np_types(all_results):\n",
    "                writer.write(item)\n",
    "        print(f\"结果已保存到文件: {results_output_path}\")\n",
    "\n",
    "\n",
    "# found_pos_token_df = pd.DataFrame({'token':found_pos_token,'token_id':found_pos_token_id})\n",
    "# found_pos_token_df.to_csv(found_token_output_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c75f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results_dataset_path = '/root/StickyToken/results/ground_truth_magic_tokens/sentence_t5_base_euclidean_distance_score_0_01.csv'\n",
    "verification_results_dataset = load_dataset('csv', data_files=verification_results_dataset_path,split='train')\n",
    "verification_results_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360984c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一个列表来存储验证结果\n",
    "verification_results = []\n",
    "\n",
    "# 遍历数据集中的每个token\n",
    "from tqdm import tqdm\n",
    "\n",
    "for row in tqdm(verification_results_dataset, desc=\"验证进度\"):\n",
    "    token_id = row['Token ID']\n",
    "    token = row['Token']\n",
    "    \n",
    "    # 使用token_verification函数进行验证\n",
    "    _, flag = token_verification(token, verification_gt_texts, verification_gt_embs, verification_contract_texts, verification_gt_metrics)\n",
    "    \n",
    "    # 将结果添加到列表中\n",
    "    verification_results.append({\n",
    "        'Token ID': token_id,\n",
    "        'Token': token,\n",
    "        'Verification Result': flag\n",
    "    })\n",
    "#结果统计\n",
    "verification_results_df = pd.DataFrame(verification_results)\n",
    "print('验证结果统计：',verification_results_df['Verification Result'].value_counts())\n",
    "\n",
    "# 将结果保存为CSV文件\n",
    "output_path = '/root/StickyToken/results/verification_results.csv'\n",
    "verification_results_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"验证结果已保存到: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de42dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da9680",
   "metadata": {},
   "outputs": [],
   "source": [
    "verification_results_df['Verification Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d4e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选出全为1的验证结果\n",
    "all_ones_results = verification_results_df[verification_results_df['Verification Result'].apply(lambda x: x == {'cosine_distance_flag': 1, 'euclidean_distance_flag': 1, 'manhattan_distance_flag': 1})]\n",
    "\n",
    "print(\"全为1的验证结果数量:\", len(all_ones_results))\n",
    "print(\"\\n前5行结果:\")\n",
    "print(all_ones_results.head())\n",
    "\n",
    "# 保存筛选后的结果\n",
    "output_path = '/root/StickyToken/results/all_ones_verification_results.csv'\n",
    "all_ones_results.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"\\n全为1的验证结果已保存到: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4716111,
     "sourceId": 8007338,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 284,
     "sourceId": 386,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 710.680066,
   "end_time": "2024-04-20T09:55:42.732272",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-20T09:43:52.052206",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "020e7ded88a847a599cc8f55538b3fd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "036b7b121ff14d7aa571117bd116302d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_967c135e3f0d46868caf9897a0af3705",
        "IPY_MODEL_fe03cdd2f2724f43815f78269c8c2313",
        "IPY_MODEL_ac686137d905410fbbece70370a22253"
       ],
       "layout": "IPY_MODEL_e9f3e4cdd2f44da1908588c7008575f2"
      }
     },
     "141926e0829d4ed88c2f672b412b7d17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1d27b316cc0642d681dbcc415e019295": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "27c5a28e38a844b1b3d1f177be12823e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_caae541cc8e44c00b4c459a2f6cd3bc4",
       "placeholder": "​",
       "style": "IPY_MODEL_1d27b316cc0642d681dbcc415e019295",
       "value": " 1.39M/1.39M [00:00&lt;00:00, 19.6MB/s]"
      }
     },
     "2e2f8d62a08945139e9831c19c9a8664": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "39a5293394f548f886964902cd37cb63": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "446950689ee447ba81c77ed168d4ca50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4657b3e4dfd242d987e1fa4d4513865c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4c6a66cec6534d39a6a3435b809b0bac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5179d58e4fdd46d180c299f13c640e7d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "53496b634e97493d8e8da91e7ae43c78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4657b3e4dfd242d987e1fa4d4513865c",
       "placeholder": "​",
       "style": "IPY_MODEL_141926e0829d4ed88c2f672b412b7d17",
       "value": " 792k/792k [00:00&lt;00:00, 4.20MB/s]"
      }
     },
     "5a22e7d92e084715a64c610a29a3aadf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6c5aba3413654fafba146797411248ae",
        "IPY_MODEL_b5f9bc85cafa4a4797bf4c75bc1c4d69",
        "IPY_MODEL_27c5a28e38a844b1b3d1f177be12823e"
       ],
       "layout": "IPY_MODEL_5d483bb997c84c19a05bfccd4a6a950c"
      }
     },
     "5d483bb997c84c19a05bfccd4a6a950c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6096f1e772fd4489a18af65118d59932": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61c2e094a5f248dbb2a2b7568c1893e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "63bfb9acfd8a40878b587f13b71b22e0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6baa4f7cb2334c7f9e71bc4bdd9f4522": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_dccf50ff239e4d038903a399739ba7d8",
        "IPY_MODEL_d35c924a823a4a60b499327207ee97f8",
        "IPY_MODEL_53496b634e97493d8e8da91e7ae43c78"
       ],
       "layout": "IPY_MODEL_5179d58e4fdd46d180c299f13c640e7d"
      }
     },
     "6c5aba3413654fafba146797411248ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_446950689ee447ba81c77ed168d4ca50",
       "placeholder": "​",
       "style": "IPY_MODEL_2e2f8d62a08945139e9831c19c9a8664",
       "value": "tokenizer.json: 100%"
      }
     },
     "7a40fe34f1584ffebefc4b14369c22cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "967c135e3f0d46868caf9897a0af3705": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_39a5293394f548f886964902cd37cb63",
       "placeholder": "​",
       "style": "IPY_MODEL_020e7ded88a847a599cc8f55538b3fd8",
       "value": "config.json: 100%"
      }
     },
     "a02e9803423845fa993fb3798523496a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a989906ab11b4ea2ac08664b7e84ad1d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "ac686137d905410fbbece70370a22253": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4c6a66cec6534d39a6a3435b809b0bac",
       "placeholder": "​",
       "style": "IPY_MODEL_a02e9803423845fa993fb3798523496a",
       "value": " 1.21k/1.21k [00:00&lt;00:00, 108kB/s]"
      }
     },
     "b0e0df905d2443de8187e50da81de5e1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b5f9bc85cafa4a4797bf4c75bc1c4d69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6096f1e772fd4489a18af65118d59932",
       "max": 1389353,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bc3cfbc86ccd4ffd879aabdb8d7d266a",
       "value": 1389353
      }
     },
     "bbd983441a5f447ca21a45a6a524d336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bc3cfbc86ccd4ffd879aabdb8d7d266a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "caae541cc8e44c00b4c459a2f6cd3bc4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d35c924a823a4a60b499327207ee97f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_63bfb9acfd8a40878b587f13b71b22e0",
       "max": 791656,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bbd983441a5f447ca21a45a6a524d336",
       "value": 791656
      }
     },
     "dccf50ff239e4d038903a399739ba7d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b0e0df905d2443de8187e50da81de5e1",
       "placeholder": "​",
       "style": "IPY_MODEL_61c2e094a5f248dbb2a2b7568c1893e7",
       "value": "spiece.model: 100%"
      }
     },
     "e9f3e4cdd2f44da1908588c7008575f2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe03cdd2f2724f43815f78269c8c2313": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a40fe34f1584ffebefc4b14369c22cf",
       "max": 1208,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a989906ab11b4ea2ac08664b7e84ad1d",
       "value": 1208
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
