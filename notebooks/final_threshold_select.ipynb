{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import sys\n",
    "sys.path.append('/root/StickyToken')\n",
    "import torch\n",
    "import gradio as gr\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import load_dataset\n",
    "from stickytoken.utils import load_verification_results\n",
    "from collections import namedtuple\n",
    "DistanceMetrics = namedtuple(\"Metrics\", [\"cosine_distance\", \"euclidean_distance\", \"manhattan_distance\"])\n",
    "from stickytoken.utils import distance_metrics, random_insert\n",
    "from stickytoken.sentence_pair import output_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_time</th>\n",
       "      <th>model_name</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>num_parameters</th>\n",
       "      <th>dataset</th>\n",
       "      <th>insert_num</th>\n",
       "      <th>model</th>\n",
       "      <th>sent_pair_num</th>\n",
       "      <th>verification_sent_pair_num</th>\n",
       "      <th>ok_tokens_num</th>\n",
       "      <th>...</th>\n",
       "      <th>caculate_vocab_token_magic_score_time</th>\n",
       "      <th>final_verification_time</th>\n",
       "      <th>vocab_embeddings_is_on_unit_sphere</th>\n",
       "      <th>wte_is_on_unit_sphere</th>\n",
       "      <th>vocab_embeddings_is_anisotropic</th>\n",
       "      <th>wte_is_anisotropic</th>\n",
       "      <th>vocab_embeddings_mean_cosine_similarity</th>\n",
       "      <th>candidates_for_verification_percentile</th>\n",
       "      <th>candidates_for_verification_threshold</th>\n",
       "      <th>candidates_for_verification_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-27 16:08:44</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>32100</td>\n",
       "      <td>110218368</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/sentence-t5-base</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>32097</td>\n",
       "      <td>...</td>\n",
       "      <td>7564.841289</td>\n",
       "      <td>7328.973902</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.795921</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.085167</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-10-28 08:00:50</td>\n",
       "      <td>sentence-t5-large</td>\n",
       "      <td>32100</td>\n",
       "      <td>335726080</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/sentence-t5-large</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>32097</td>\n",
       "      <td>...</td>\n",
       "      <td>18250.417344</td>\n",
       "      <td>18864.129460</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.763351</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.076265</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-10-28 12:59:15</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>30522</td>\n",
       "      <td>22713216</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>23699</td>\n",
       "      <td>...</td>\n",
       "      <td>5054.268309</td>\n",
       "      <td>4728.382494</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.204199</td>\n",
       "      <td>474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           record_time         model_name  vocab_size  num_parameters  \\\n",
       "0  2024-10-27 16:08:44   sentence-t5-base       32100       110218368   \n",
       "1  2024-10-28 08:00:50  sentence-t5-large       32100       335726080   \n",
       "2  2024-10-28 12:59:15   all-MiniLM-L6-v2       30522        22713216   \n",
       "\n",
       "                                             dataset  insert_num  \\\n",
       "0  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...           8   \n",
       "1  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...           8   \n",
       "2  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...           8   \n",
       "\n",
       "                                           model  sent_pair_num  \\\n",
       "0   G:/hf/sentence-transformers/sentence-t5-base              5   \n",
       "1  G:/hf/sentence-transformers/sentence-t5-large              5   \n",
       "2   G:/hf/sentence-transformers/all-MiniLM-L6-v2              5   \n",
       "\n",
       "   verification_sent_pair_num  ok_tokens_num  ...  \\\n",
       "0                         250          32097  ...   \n",
       "1                         250          32097  ...   \n",
       "2                         250          23699  ...   \n",
       "\n",
       "   caculate_vocab_token_magic_score_time  final_verification_time  \\\n",
       "0                            7564.841289              7328.973902   \n",
       "1                           18250.417344             18864.129460   \n",
       "2                            5054.268309              4728.382494   \n",
       "\n",
       "   vocab_embeddings_is_on_unit_sphere  wte_is_on_unit_sphere  \\\n",
       "0                                True                  False   \n",
       "1                                True                  False   \n",
       "2                                True                  False   \n",
       "\n",
       "   vocab_embeddings_is_anisotropic  wte_is_anisotropic  \\\n",
       "0                             True                True   \n",
       "1                             True                True   \n",
       "2                             True                True   \n",
       "\n",
       "   vocab_embeddings_mean_cosine_similarity  \\\n",
       "0                                 0.795921   \n",
       "1                                 0.763351   \n",
       "2                                 0.199846   \n",
       "\n",
       "   candidates_for_verification_percentile  \\\n",
       "0                                     2.0   \n",
       "1                                     2.0   \n",
       "2                                     2.0   \n",
       "\n",
       "   candidates_for_verification_threshold  candidates_for_verification_num  \n",
       "0                              56.085167                              642  \n",
       "1                              49.076265                              642  \n",
       "2                              49.204199                              474  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/root/StickyToken/results/experiment_information.json\"\n",
    "try:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        experiement_record = json.load(f)\n",
    "except (FileNotFoundError, json.JSONDecodeError):\n",
    "    experiement_record = []\n",
    "experiement_record_df = pd.DataFrame(experiement_record)\n",
    "model_list = experiement_record_df[\"model_name\"].tolist()\n",
    "experiement_record_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sentence-t5-base',\n",
       " 'sentence-t5-large',\n",
       " 'all-MiniLM-L6-v2',\n",
       " 'bge-base-en-v1.5',\n",
       " 'bge-small-en-v1.5',\n",
       " 'all-mpnet-base-v2',\n",
       " 'sentence-t5-xl',\n",
       " 'e5-small',\n",
       " 'e5-base',\n",
       " 'e5-large',\n",
       " 'bge-large-en-v1.5',\n",
       " 'gtr-t5-base',\n",
       " 'gte-base',\n",
       " 'gte-small',\n",
       " 'gte-base-en-v1.5',\n",
       " 'gtr-t5-large',\n",
       " 'gte-large',\n",
       " 'gte-large-en-v1.5',\n",
       " 'gtr-t5-xl',\n",
       " 'GritLM-7B',\n",
       " 'sentence-t5-xxl',\n",
       " 'sup-simcse-bert-base-uncased',\n",
       " 'sup-simcse-bert-large-uncased',\n",
       " 'sup-simcse-roberta-base',\n",
       " 'sup-simcse-roberta-large',\n",
       " 'gtr-t5-xxl',\n",
       " 'e5-mistral-7b-instruct',\n",
       " 'nomic-embed-text-v1.5',\n",
       " 'nomic-embed-text-v1',\n",
       " 'SFR-Embedding-Mistral',\n",
       " 'SFR-Embedding-2_R',\n",
       " 'instructor-base',\n",
       " 'gte-Qwen2-7B-instruct',\n",
       " 'gte-Qwen2-1.5B-instruct',\n",
       " 'UAE-Large-V1',\n",
       " 'instructor-xl',\n",
       " 'instructor-large']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-t5-base\n",
      "sentence-t5-large\n",
      "all-MiniLM-L6-v2\n",
      "bge-base-en-v1.5\n",
      "bge-small-en-v1.5\n",
      "all-mpnet-base-v2\n",
      "sentence-t5-xl\n",
      "e5-small\n",
      "e5-base\n",
      "e5-large\n",
      "bge-large-en-v1.5\n",
      "gtr-t5-base\n",
      "gte-base\n",
      "gte-small\n",
      "gte-base-en-v1.5\n",
      "gtr-t5-large\n",
      "gte-large\n",
      "gte-large-en-v1.5\n",
      "gtr-t5-xl\n",
      "GritLM-7B\n",
      "sentence-t5-xxl\n",
      "sup-simcse-bert-base-uncased\n",
      "sup-simcse-bert-large-uncased\n",
      "sup-simcse-roberta-base\n",
      "sup-simcse-roberta-large\n",
      "gtr-t5-xxl\n",
      "e5-mistral-7b-instruct\n",
      "nomic-embed-text-v1.5\n",
      "nomic-embed-text-v1\n",
      "SFR-Embedding-Mistral\n",
      "SFR-Embedding-2_R\n",
      "instructor-base\n",
      "gte-Qwen2-7B-instruct\n",
      "gte-Qwen2-1.5B-instruct\n",
      "UAE-Large-V1\n",
      "instructor-xl\n",
      "instructor-large\n"
     ]
    }
   ],
   "source": [
    "all_verifications_df = pd.DataFrame()\n",
    "\n",
    "for model_name in model_list:\n",
    "    print(model_name)\n",
    "    verification_results = load_verification_results(model_name)\n",
    "    verification_results_df = pd.DataFrame(verification_results).transpose()\n",
    "    # 只取出‘verification’列非空的数据\n",
    "    verifications_df = verification_results_df[verification_results_df['verification'].notnull()].copy()\n",
    "    # 添加一个列，列名为‘model’，值为model_name\n",
    "    verifications_df.loc[:, 'model'] = model_name\n",
    "    all_verifications_df = pd.concat([all_verifications_df, verifications_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verifications_df.to_csv(\"all_model_verification_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>raw_vocab</th>\n",
       "      <th>category</th>\n",
       "      <th>decoded</th>\n",
       "      <th>metrics</th>\n",
       "      <th>main_metric</th>\n",
       "      <th>metric_names</th>\n",
       "      <th>main_metric_name</th>\n",
       "      <th>verification</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>magic</th>\n",
       "      <th>reencoded_ids</th>\n",
       "      <th>reencoded</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>OK_SPECIAL</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>[97.830365, 106.041251, 137.680227]</td>\n",
       "      <td>97.830365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1.0000000000000122, 0.9985454545454666, 0.998...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>317</td>\n",
       "      <td>▁think</td>\n",
       "      <td>OK</td>\n",
       "      <td>think</td>\n",
       "      <td>[56.713909, 59.961645, 62.589155]</td>\n",
       "      <td>56.713909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.8920000000000091, 0.8894545454545545, 0.878...</td>\n",
       "      <td>0.892</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>344</td>\n",
       "      <td>▁between</td>\n",
       "      <td>OK</td>\n",
       "      <td>between</td>\n",
       "      <td>[56.294466, 59.531261, 63.816119]</td>\n",
       "      <td>56.294466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.8803636363636451, 0.8789090909090996, 0.858...</td>\n",
       "      <td>0.880364</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     i raw_vocab    category   decoded                              metrics  \\\n",
       "0    1      </s>  OK_SPECIAL      </s>  [97.830365, 106.041251, 137.680227]   \n",
       "1  317    ▁think          OK     think    [56.713909, 59.961645, 62.589155]   \n",
       "2  344  ▁between          OK   between    [56.294466, 59.531261, 63.816119]   \n",
       "\n",
       "  main_metric metric_names main_metric_name  \\\n",
       "0   97.830365          NaN              NaN   \n",
       "1   56.713909          NaN              NaN   \n",
       "2   56.294466          NaN              NaN   \n",
       "\n",
       "                                        verification  max_prob  \\\n",
       "0  [1.0000000000000122, 0.9985454545454666, 0.998...       1.0   \n",
       "1  [0.8920000000000091, 0.8894545454545545, 0.878...     0.892   \n",
       "2  [0.8803636363636451, 0.8789090909090996, 0.858...  0.880364   \n",
       "\n",
       "             magic reencoded_ids reencoded             model  \n",
       "0  strong_verified           NaN       NaN  sentence-t5-base  \n",
       "1  strong_verified           NaN       NaN  sentence-t5-base  \n",
       "2  strong_verified           NaN       NaN  sentence-t5-base  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verifications_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_verifications_df只保留model列和max_prob列,并且max_prob列的值转化为float类型\n",
    "all_verifications_df = all_verifications_df[['model', 'max_prob']]\n",
    "all_verifications_df['max_prob'] = all_verifications_df['max_prob'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>max_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>0.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>0.880364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  max_prob\n",
       "0  sentence-t5-base  1.000000\n",
       "1  sentence-t5-base  0.892000\n",
       "2  sentence-t5-base  0.880364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verifications_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptitprince as pt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义箱线图的属性\n",
    "medianprops = {\"linewidth\": 1.5, \"color\": \"#a9a9a9\", \"solid_capstyle\": \"butt\"}\n",
    "boxprops = {\"linewidth\": 1.5, \"color\": \"#a9a9a9\"}\n",
    "\n",
    "# 获取模型列表\n",
    "MODELS = model_list\n",
    "COLORS = [\n",
    "    \"#DE7833\", \"#912C2C\", \"#F2BB6B\", \"#C2ABC8\", \"#329845\", \"#AED185\", \n",
    "    \"#276C9E\", \"#A3C9D5\", \"#FF5733\", \"#33FF57\", \"#3357FF\", \"#FF33A1\", \n",
    "    \"#33FFA1\", \"#A133FF\", \"#FF8C33\", \"#33FF8C\", \"#8C33FF\", \"#FF338C\",\n",
    "    \"#FFB533\", \"#33FFB5\", \"#B533FF\", \"#FF33B5\", \"#B5FF33\", \"#33B5FF\",\n",
    "    \"#FF6633\", \"#33FF66\", \"#6633FF\", \"#FF3366\", \"#66FF33\", \"#3366FF\",\n",
    "        \"#DE7833\", \"#912C2C\", \"#F2BB6B\", \"#C2ABC8\", \"#329845\", \"#AED185\", \n",
    "    \"#276C9E\", \"#A3C9D5\", \"#FF5733\", \"#33FF57\", \"#3357FF\", \"#FF33A1\", \n",
    "    \"#33FFA1\", \"#A133FF\", \"#FF8C33\", \"#33FF8C\", \"#8C33FF\", \"#FF338C\",\n",
    "    \"#FFB533\", \"#33FFB5\", \"#B533FF\", \"#FF33B5\", \"#B5FF33\", \"#33B5FF\",\n",
    "    \"#FF6633\", \"#33FF66\", \"#6633FF\", \"#FF3366\", \"#66FF33\", \"#3366FF\"\n",
    "][:len(MODELS)]\n",
    "\n",
    "# 创建自定义大小的图形\n",
    "fig, ax = plt.subplots(figsize=(16, 24))\n",
    "\n",
    "# 绘制半小提琴图\n",
    "pt.half_violinplot(\n",
    "    x='max_prob', y='model', scale='area', palette=COLORS, \n",
    "    inner=None, data=all_verifications_df, width=1, ax=ax\n",
    ")\n",
    "\n",
    "# 遍历模型\n",
    "for i, model in enumerate(MODELS):\n",
    "    # 筛选数据\n",
    "    data = all_verifications_df[all_verifications_df[\"model\"] == model]\n",
    "    # 在垂直轴上抖动数值\n",
    "    y = i + np.random.uniform(high=0.2, size=len(data))\n",
    "    # 选择水平轴的数值\n",
    "    x = data[\"max_prob\"]\n",
    "    # 使用scatter方法添加雨点，点更小更轻\n",
    "    ax.scatter(x, y, color=COLORS[i], alpha=0.1, s=15)\n",
    "\n",
    "# 生成数组列表\n",
    "boxplot_data = [\n",
    "    all_verifications_df[all_verifications_df[\"model\"] == model][\"max_prob\"].values \n",
    "    for model in MODELS\n",
    "]\n",
    "\n",
    "# 调整箱线图的位置\n",
    "SHIFT = 0.1\n",
    "POSITIONS = [i + SHIFT for i in range(len(MODELS))]\n",
    "\n",
    "ax.boxplot(\n",
    "    boxplot_data, \n",
    "    vert=False, \n",
    "    positions=POSITIONS, \n",
    "    manage_ticks=False,\n",
    "    showfliers=True,  # 不显示超出箱线图的异常值\n",
    "    showcaps=False,    # 不显示箱线图的顶端和底端\n",
    "    medianprops=medianprops,\n",
    "    whiskerprops=boxprops,\n",
    "    boxprops=boxprops,\n",
    "    widths=0.25\n",
    ")\n",
    "\n",
    "# 添加标签和标题\n",
    "ax.set_xlabel(\"Max Probability Distribution\", fontsize=12)\n",
    "ax.set_ylabel(\"Model\", fontsize=12)\n",
    "\n",
    "# 修改刻度标签的大小\n",
    "ax.tick_params(labelsize=13)\n",
    "# fig.savefig(r'G:\\juchiyun2024-11-14\\ckx_ws\\StickyToken\\fig\\max_prob_distribution.pdf', format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_time</th>\n",
       "      <th>model_name</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>num_parameters</th>\n",
       "      <th>dataset</th>\n",
       "      <th>insert_num</th>\n",
       "      <th>model</th>\n",
       "      <th>sent_pair_num</th>\n",
       "      <th>verification_sent_pair_num</th>\n",
       "      <th>ok_tokens_num</th>\n",
       "      <th>...</th>\n",
       "      <th>caculate_vocab_token_magic_score_time</th>\n",
       "      <th>final_verification_time</th>\n",
       "      <th>vocab_embeddings_is_on_unit_sphere</th>\n",
       "      <th>wte_is_on_unit_sphere</th>\n",
       "      <th>vocab_embeddings_is_anisotropic</th>\n",
       "      <th>wte_is_anisotropic</th>\n",
       "      <th>vocab_embeddings_mean_cosine_similarity</th>\n",
       "      <th>candidates_for_verification_percentile</th>\n",
       "      <th>candidates_for_verification_threshold</th>\n",
       "      <th>candidates_for_verification_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-10-27 16:08:44</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>32100</td>\n",
       "      <td>110218368</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/sentence-t5-base</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>32097</td>\n",
       "      <td>...</td>\n",
       "      <td>7564.841289</td>\n",
       "      <td>7328.973902</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.795921</td>\n",
       "      <td>2.0</td>\n",
       "      <td>56.085167</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           record_time        model_name  vocab_size  num_parameters  \\\n",
       "0  2024-10-27 16:08:44  sentence-t5-base       32100       110218368   \n",
       "\n",
       "                                             dataset  insert_num  \\\n",
       "0  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...           8   \n",
       "\n",
       "                                          model  sent_pair_num  \\\n",
       "0  G:/hf/sentence-transformers/sentence-t5-base              5   \n",
       "\n",
       "   verification_sent_pair_num  ok_tokens_num  ...  \\\n",
       "0                         250          32097  ...   \n",
       "\n",
       "   caculate_vocab_token_magic_score_time  final_verification_time  \\\n",
       "0                            7564.841289              7328.973902   \n",
       "\n",
       "   vocab_embeddings_is_on_unit_sphere  wte_is_on_unit_sphere  \\\n",
       "0                                True                  False   \n",
       "\n",
       "   vocab_embeddings_is_anisotropic  wte_is_anisotropic  \\\n",
       "0                             True                True   \n",
       "\n",
       "   vocab_embeddings_mean_cosine_similarity  \\\n",
       "0                                 0.795921   \n",
       "\n",
       "   candidates_for_verification_percentile  \\\n",
       "0                                     2.0   \n",
       "\n",
       "   candidates_for_verification_threshold  candidates_for_verification_num  \n",
       "0                              56.085167                              642  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiement_record_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i</th>\n",
       "      <th>raw_vocab</th>\n",
       "      <th>category</th>\n",
       "      <th>decoded</th>\n",
       "      <th>metrics</th>\n",
       "      <th>main_metric</th>\n",
       "      <th>metric_names</th>\n",
       "      <th>main_metric_name</th>\n",
       "      <th>verification</th>\n",
       "      <th>max_prob</th>\n",
       "      <th>magic</th>\n",
       "      <th>reencoded_ids</th>\n",
       "      <th>reencoded</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>OK_SPECIAL</td>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>[97.830365, 106.041251, 137.680227]</td>\n",
       "      <td>97.830365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[1.0000000000000122, 0.9985454545454666, 0.998...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2226</td>\n",
       "      <td>▁tip</td>\n",
       "      <td>OK</td>\n",
       "      <td>tip</td>\n",
       "      <td>[56.475278, 59.811531, 63.237172]</td>\n",
       "      <td>56.475278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.9418181818181923, 0.9392727272727377, 0.918...</td>\n",
       "      <td>0.941818</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6397</td>\n",
       "      <td>▁br</td>\n",
       "      <td>OK</td>\n",
       "      <td>br</td>\n",
       "      <td>[61.626297, 65.118991, 69.23426]</td>\n",
       "      <td>61.626297</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.9352727272727376, 0.932727272727283, 0.9123...</td>\n",
       "      <td>0.935273</td>\n",
       "      <td>strong_verified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sentence-t5-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      i raw_vocab    category decoded                              metrics  \\\n",
       "0     1      </s>  OK_SPECIAL    </s>  [97.830365, 106.041251, 137.680227]   \n",
       "1  2226      ▁tip          OK     tip    [56.475278, 59.811531, 63.237172]   \n",
       "2  6397       ▁br          OK      br     [61.626297, 65.118991, 69.23426]   \n",
       "\n",
       "  main_metric metric_names main_metric_name  \\\n",
       "0   97.830365          NaN              NaN   \n",
       "1   56.475278          NaN              NaN   \n",
       "2   61.626297          NaN              NaN   \n",
       "\n",
       "                                        verification  max_prob  \\\n",
       "0  [1.0000000000000122, 0.9985454545454666, 0.998...       1.0   \n",
       "1  [0.9418181818181923, 0.9392727272727377, 0.918...  0.941818   \n",
       "2  [0.9352727272727376, 0.932727272727283, 0.9123...  0.935273   \n",
       "\n",
       "             magic reencoded_ids reencoded             model  \n",
       "0  strong_verified           NaN       NaN  sentence-t5-base  \n",
       "1  strong_verified           NaN       NaN  sentence-t5-base  \n",
       "2  strong_verified           NaN       NaN  sentence-t5-base  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verifications_statics_df = pd.DataFrame()\n",
    "\n",
    "for model_name in model_list:\n",
    "    verification_results = load_verification_results(model_name)\n",
    "    verification_results_df = pd.DataFrame(verification_results).transpose()\n",
    "    # 只取出‘verification’列非空的数据\n",
    "    verifications_df = verification_results_df[verification_results_df['verification'].notnull()].copy()\n",
    "    # 计算所有行的列表中第一个元素的中位数和四分位数\n",
    "    first_elements = verifications_df['verification'].apply(lambda x: x[0])\n",
    "    Q1 = first_elements.quantile(0.25)\n",
    "    median = first_elements.median()\n",
    "    Q3 = first_elements.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # 计算上下边界\n",
    "    # lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 0.7 * IQR\n",
    "\n",
    "    # print(f\"Q1: {Q1}, Median: {median}, Q3: {Q3}, IQR: {IQR}\")\n",
    "    # print(f\"Lower Bound: {lower_bound}, Upper Bound: {upper_bound}\")\n",
    "\n",
    "    # 找出超过上边界的点和个数\n",
    "    outliers = first_elements[first_elements > upper_bound]\n",
    "    outliers_count = outliers.count()\n",
    "\n",
    "    # 找出verifications_df行中超出上边界的行\n",
    "    outliers_df = verifications_df[first_elements > upper_bound].copy()\n",
    "    # 检查outliers_df是否为空\n",
    "    if not outliers_df.empty:\n",
    "        # 如果非空，添加一个列，列名为‘model’，值为model_name\n",
    "        outliers_df.loc[:, 'model'] = model_name\n",
    "    all_verifications_statics_df = pd.concat([all_verifications_statics_df, outliers_df], ignore_index=True)\n",
    "\n",
    "    # 将统计信息添加到现有的experiement_record_df中\n",
    "    experiement_record_df.loc[experiement_record_df['model_name'] == model_name, ['Q1', 'Median', 'Q3', 'Upper Bound', 'sticky token count']] = [\n",
    "        Q1, median, Q3, upper_bound, outliers_count\n",
    "    ]\n",
    "\n",
    "all_verifications_statics_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verifications_statics_df.to_csv(\"../results/final_all_models_sticky_tokens.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>record_time</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>num_parameters</th>\n",
       "      <th>dataset</th>\n",
       "      <th>insert_num</th>\n",
       "      <th>model</th>\n",
       "      <th>sent_pair_num</th>\n",
       "      <th>verification_sent_pair_num</th>\n",
       "      <th>ok_tokens_num</th>\n",
       "      <th>...</th>\n",
       "      <th>wte_is_anisotropic</th>\n",
       "      <th>vocab_embeddings_mean_cosine_similarity</th>\n",
       "      <th>candidates_for_verification_percentile</th>\n",
       "      <th>candidates_for_verification_threshold</th>\n",
       "      <th>candidates_for_verification_num</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Median</th>\n",
       "      <th>Q3</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>sticky token count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>2024-10-28 12:59:15</td>\n",
       "      <td>30522</td>\n",
       "      <td>22713216</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>23699</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.199846</td>\n",
       "      <td>2.0</td>\n",
       "      <td>49.204199</td>\n",
       "      <td>474</td>\n",
       "      <td>0.748727</td>\n",
       "      <td>0.775273</td>\n",
       "      <td>0.796727</td>\n",
       "      <td>0.830327</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all-mpnet-base-v2</td>\n",
       "      <td>2024-10-28 14:40:12</td>\n",
       "      <td>30527</td>\n",
       "      <td>109486464</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>23700</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.187637</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.803801</td>\n",
       "      <td>474</td>\n",
       "      <td>0.714364</td>\n",
       "      <td>0.735091</td>\n",
       "      <td>0.756364</td>\n",
       "      <td>0.785764</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sup-simcse-bert-base-uncased</td>\n",
       "      <td>2024-11-02 12:11:26</td>\n",
       "      <td>30522</td>\n",
       "      <td>109482240</td>\n",
       "      <td>[G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...</td>\n",
       "      <td>8</td>\n",
       "      <td>G:/hf/princeton-nlp/sup-simcse-bert-base-uncased</td>\n",
       "      <td>5</td>\n",
       "      <td>250</td>\n",
       "      <td>23699</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.586649</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.052514</td>\n",
       "      <td>474</td>\n",
       "      <td>0.687818</td>\n",
       "      <td>0.717818</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>0.785027</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model_name          record_time  vocab_size  \\\n",
       "0              all-MiniLM-L6-v2  2024-10-28 12:59:15       30522   \n",
       "1             all-mpnet-base-v2  2024-10-28 14:40:12       30527   \n",
       "2  sup-simcse-bert-base-uncased  2024-11-02 12:11:26       30522   \n",
       "\n",
       "   num_parameters                                            dataset  \\\n",
       "0        22713216  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...   \n",
       "1       109486464  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...   \n",
       "2       109482240  [G:/juchiyun2024-11-14/hf_dataset/mteb/sts13-s...   \n",
       "\n",
       "   insert_num                                             model  \\\n",
       "0           8      G:/hf/sentence-transformers/all-MiniLM-L6-v2   \n",
       "1           8     G:/hf/sentence-transformers/all-mpnet-base-v2   \n",
       "2           8  G:/hf/princeton-nlp/sup-simcse-bert-base-uncased   \n",
       "\n",
       "   sent_pair_num  verification_sent_pair_num  ok_tokens_num  ...  \\\n",
       "0              5                         250          23699  ...   \n",
       "1              5                         250          23700  ...   \n",
       "2              5                         250          23699  ...   \n",
       "\n",
       "   wte_is_anisotropic  vocab_embeddings_mean_cosine_similarity  \\\n",
       "0                True                                 0.199846   \n",
       "1                True                                 0.187637   \n",
       "2                True                                 0.586649   \n",
       "\n",
       "   candidates_for_verification_percentile  \\\n",
       "0                                     2.0   \n",
       "1                                     2.0   \n",
       "2                                     2.0   \n",
       "\n",
       "   candidates_for_verification_threshold  candidates_for_verification_num  \\\n",
       "0                              49.204199                              474   \n",
       "1                              22.803801                              474   \n",
       "2                              36.052514                              474   \n",
       "\n",
       "         Q1    Median        Q3  Upper Bound  sticky token count  \n",
       "0  0.748727  0.775273  0.796727     0.830327                21.0  \n",
       "1  0.714364  0.735091  0.756364     0.785764                24.0  \n",
       "2  0.687818  0.717818  0.745000     0.785027                22.0  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiement_record_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sticky_token_df = experiement_record_df[['model_name','vocab_size' ,'sticky token count']]\n",
    "final_sticky_token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sticky_token_list(model_name):\n",
    "    sticky_tokens_of_all_models_df = pd.read_csv(\"../results/final_all_models_sticky_tokens.csv\")\n",
    "    # 先筛选出对应模型的行\n",
    "    sticky_token_column = sticky_tokens_of_all_models_df[sticky_tokens_of_all_models_df['model'] == model_name]\n",
    "    # 按照main_metric从高到低排序\n",
    "    sticky_token_column = sticky_token_column.sort_values(by='main_metric', ascending=False)\n",
    "    # 获取排序后的raw_vocab列表\n",
    "    sticky_token_list = sticky_token_column['raw_vocab'].to_list()\n",
    "    return sticky_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['（',\n",
       " 'textbook',\n",
       " 'h₂o',\n",
       " 'satisfy',\n",
       " 'trajectory',\n",
       " 'julio',\n",
       " 'functioning',\n",
       " '[CLS]',\n",
       " '₂',\n",
       " 'gambia',\n",
       " 'defendant',\n",
       " '？',\n",
       " '{',\n",
       " 'functioned',\n",
       " 'imaginative',\n",
       " 'cultivated',\n",
       " 'う',\n",
       " 'intelligent',\n",
       " 'oskar',\n",
       " 'whereupon',\n",
       " 'intended']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sticky_token_list(model_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>num_parameters</th>\n",
       "      <th>vocab_size</th>\n",
       "      <th>ok_tokens_num</th>\n",
       "      <th>candidates_for_verification_num</th>\n",
       "      <th>sticky_token_count</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>23M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>21</td>\n",
       "      <td>（, textbook, h₂o, satisfy, trajectory, julio, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all-mpnet-base-v2</td>\n",
       "      <td>109M</td>\n",
       "      <td>30527</td>\n",
       "      <td>23700</td>\n",
       "      <td>474</td>\n",
       "      <td>24</td>\n",
       "      <td>00, adversary, intended, ambiguous, cooked, た,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sup-simcse-bert-base-uncased</td>\n",
       "      <td>109M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>22</td>\n",
       "      <td>203, ?, [SEP], ロ, game, 640, り, victories, cal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sup-simcse-bert-large-uncased</td>\n",
       "      <td>335M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "      <td>', ;, contestants, accidental, ɔ, continents, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sup-simcse-roberta-base</td>\n",
       "      <td>125M</td>\n",
       "      <td>50265</td>\n",
       "      <td>49894</td>\n",
       "      <td>998</td>\n",
       "      <td>27</td>\n",
       "      <td>ĠThere, There, Ġthere, &lt;/s&gt;, there, ĠTHERE, ĠE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sup-simcse-roberta-large</td>\n",
       "      <td>355M</td>\n",
       "      <td>50265</td>\n",
       "      <td>49894</td>\n",
       "      <td>998</td>\n",
       "      <td>25</td>\n",
       "      <td>Discussion, ĠâĢĭ, ĠSubjects, Topic, Ġ?, .-, Ġs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sentence-t5-base</td>\n",
       "      <td>110M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>21</td>\n",
       "      <td>&lt;/s&gt;, lucrarea, ▁grains, ▁photographed, ▁sport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence-t5-large</td>\n",
       "      <td>336M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>30</td>\n",
       "      <td>&lt;/s&gt;, ▁»., &lt;extra_id_27&gt;, ▁Comment, ▁Ribbon, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sentence-t5-xl</td>\n",
       "      <td>1242M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>34</td>\n",
       "      <td>&lt;/s&gt;, &lt;extra_id_0&gt;, &lt;extra_id_27&gt;, ▁velvet, ▁c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sentence-t5-xxl</td>\n",
       "      <td>4866M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>22</td>\n",
       "      <td>&lt;/s&gt;, ▁consacré, &lt;extra_id_27&gt;, ▁hashtag, ▁hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gtr-t5-base</td>\n",
       "      <td>110M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>16</td>\n",
       "      <td>&lt;/s&gt;, lucrarea, ▁Someone, &lt;extra_id_26&gt;, ▁happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gtr-t5-large</td>\n",
       "      <td>336M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>14</td>\n",
       "      <td>▁»., &lt;/s&gt;, &lt;extra_id_27&gt;, &lt;extra_id_25&gt;, ▁supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gtr-t5-xl</td>\n",
       "      <td>1242M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;/s&gt;, &lt;extra_id_0&gt;, &lt;extra_id_9&gt;, &lt;extra_id_27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gtr-t5-xxl</td>\n",
       "      <td>4866M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;/s&gt;, ▁consacré, ▁shortly, Pourtant, ▁indeed, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>instructor-base</td>\n",
       "      <td>110M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;/s&gt;, lucrarea, &lt;extra_id_26&gt;, ▁somewhere, &lt;ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>instructor-large</td>\n",
       "      <td>336M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;/s&gt;, ▁»., &lt;extra_id_27&gt;, ▁waiting, ▁exhausted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>instructor-xl</td>\n",
       "      <td>1242M</td>\n",
       "      <td>32100</td>\n",
       "      <td>32097</td>\n",
       "      <td>642</td>\n",
       "      <td>8</td>\n",
       "      <td>&lt;/s&gt;, &lt;extra_id_0&gt;, &lt;extra_id_9&gt;, &lt;extra_id_27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>e5-small</td>\n",
       "      <td>33M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>17</td>\n",
       "      <td>[SEP], exhibiting, occurring, pretended, behav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>e5-base</td>\n",
       "      <td>109M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>11</td>\n",
       "      <td>generating, absorbing, heating, carpet, human,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>e5-large</td>\n",
       "      <td>335M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>21</td>\n",
       "      <td>ರ, ⇄, 扌, [SEP], ∅, ⺩, [MASK], ⺼, [PAD], 都, [CL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>e5-mistral-7b-instruct</td>\n",
       "      <td>7111M</td>\n",
       "      <td>32000</td>\n",
       "      <td>31747</td>\n",
       "      <td>635</td>\n",
       "      <td>31</td>\n",
       "      <td>▁sont, ▁peut, ▁много, жду, ▁испо, ▁которы, ци,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bge-small-en-v1.5</td>\n",
       "      <td>33M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>18</td>\n",
       "      <td>[, brought, ð, deposited, december, climax, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bge-base-en-v1.5</td>\n",
       "      <td>109M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>20</td>\n",
       "      <td>neighbouring, ？, witnessed, granting, 。, proce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>bge-large-en-v1.5</td>\n",
       "      <td>335M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>15</td>\n",
       "      <td>actively, intended, intercepted, intentional, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>UAE-Large-V1</td>\n",
       "      <td>335M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>14</td>\n",
       "      <td>[SEP], ɔ, ո, occurring, having, intercept, ʊ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>nomic-embed-text-v1</td>\n",
       "      <td>137M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>12</td>\n",
       "      <td>[CLS], [MASK], ¦, polling, 勝, [SEP], qualifier...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>nomic-embed-text-v1.5</td>\n",
       "      <td>137M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>9</td>\n",
       "      <td>[CLS], [MASK], [SEP], cerambycidae, ～, etienne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>gte-small</td>\n",
       "      <td>33M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>15</td>\n",
       "      <td>[SEP], [CLS], treacherous, 2nd, peacefully, 水,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>gte-base</td>\n",
       "      <td>109M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>18</td>\n",
       "      <td>[SEP], [MASK], hotspur, [CLS], aroused, 3a, mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>gte-large</td>\n",
       "      <td>335M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>18</td>\n",
       "      <td>ٹ, 1st, 30th, mcgrath, rendering, 15th, ɑ, 33r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>gte-base-en-v1.5</td>\n",
       "      <td>137M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>20</td>\n",
       "      <td>[CLS], ~, ₆, ₎, ,, ₍, ∞, ₃, ■, ⊕, ⁴, ⇌, ᄌ, ℓ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gte-large-en-v1.5</td>\n",
       "      <td>434M</td>\n",
       "      <td>30522</td>\n",
       "      <td>23699</td>\n",
       "      <td>474</td>\n",
       "      <td>17</td>\n",
       "      <td>扌, multiplied, ː, ∧, ʑ, ‿, ♯, ^, factual, ɪ, h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gte-Qwen2-1.5B-instruct</td>\n",
       "      <td>1543M</td>\n",
       "      <td>151643</td>\n",
       "      <td>147848</td>\n",
       "      <td>2326</td>\n",
       "      <td>5</td>\n",
       "      <td>Ġthru, Ġgifted, Ġupfront, Ġportraying, Ġawkward</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gte-Qwen2-7B-instruct</td>\n",
       "      <td>7069M</td>\n",
       "      <td>151643</td>\n",
       "      <td>147848</td>\n",
       "      <td>2957</td>\n",
       "      <td>103</td>\n",
       "      <td>Ġanon, Ġcommenting, Ġsolver, ĠChecking, ĠSteer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GritLM-7B</td>\n",
       "      <td>7111M</td>\n",
       "      <td>32000</td>\n",
       "      <td>31747</td>\n",
       "      <td>635</td>\n",
       "      <td>17</td>\n",
       "      <td>▁adventures, ▁promoting, ▁nine, ▁folks, ▁villa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>SFR-Embedding-2_R</td>\n",
       "      <td>7111M</td>\n",
       "      <td>32000</td>\n",
       "      <td>31716</td>\n",
       "      <td>444</td>\n",
       "      <td>2</td>\n",
       "      <td>zeichnet, ▁scales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>SFR-Embedding-Mistral</td>\n",
       "      <td>7111M</td>\n",
       "      <td>32000</td>\n",
       "      <td>31716</td>\n",
       "      <td>635</td>\n",
       "      <td>46</td>\n",
       "      <td>▁которы, ▁годи, ▁Jahrhund, ▁который, ▁которых,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_name num_parameters  vocab_size  ok_tokens_num  \\\n",
       "0                all-MiniLM-L6-v2            23M       30522          23699   \n",
       "1               all-mpnet-base-v2           109M       30527          23700   \n",
       "2    sup-simcse-bert-base-uncased           109M       30522          23699   \n",
       "3   sup-simcse-bert-large-uncased           335M       30522          23699   \n",
       "4         sup-simcse-roberta-base           125M       50265          49894   \n",
       "5        sup-simcse-roberta-large           355M       50265          49894   \n",
       "6                sentence-t5-base           110M       32100          32097   \n",
       "7               sentence-t5-large           336M       32100          32097   \n",
       "8                  sentence-t5-xl          1242M       32100          32097   \n",
       "9                 sentence-t5-xxl          4866M       32100          32097   \n",
       "10                    gtr-t5-base           110M       32100          32097   \n",
       "11                   gtr-t5-large           336M       32100          32097   \n",
       "12                      gtr-t5-xl          1242M       32100          32097   \n",
       "13                     gtr-t5-xxl          4866M       32100          32097   \n",
       "14                instructor-base           110M       32100          32097   \n",
       "15               instructor-large           336M       32100          32097   \n",
       "16                  instructor-xl          1242M       32100          32097   \n",
       "17                       e5-small            33M       30522          23699   \n",
       "18                        e5-base           109M       30522          23699   \n",
       "19                       e5-large           335M       30522          23699   \n",
       "20         e5-mistral-7b-instruct          7111M       32000          31747   \n",
       "21              bge-small-en-v1.5            33M       30522          23699   \n",
       "22               bge-base-en-v1.5           109M       30522          23699   \n",
       "23              bge-large-en-v1.5           335M       30522          23699   \n",
       "24                   UAE-Large-V1           335M       30522          23699   \n",
       "25            nomic-embed-text-v1           137M       30522          23699   \n",
       "26          nomic-embed-text-v1.5           137M       30522          23699   \n",
       "27                      gte-small            33M       30522          23699   \n",
       "28                       gte-base           109M       30522          23699   \n",
       "29                      gte-large           335M       30522          23699   \n",
       "30               gte-base-en-v1.5           137M       30522          23699   \n",
       "31              gte-large-en-v1.5           434M       30522          23699   \n",
       "32        gte-Qwen2-1.5B-instruct          1543M      151643         147848   \n",
       "33          gte-Qwen2-7B-instruct          7069M      151643         147848   \n",
       "34                      GritLM-7B          7111M       32000          31747   \n",
       "35              SFR-Embedding-2_R          7111M       32000          31716   \n",
       "36          SFR-Embedding-Mistral          7111M       32000          31716   \n",
       "\n",
       "    candidates_for_verification_num  sticky_token_count  \\\n",
       "0                               474                  21   \n",
       "1                               474                  24   \n",
       "2                               474                  22   \n",
       "3                               474                  11   \n",
       "4                               998                  27   \n",
       "5                               998                  25   \n",
       "6                               642                  21   \n",
       "7                               642                  30   \n",
       "8                               642                  34   \n",
       "9                               642                  22   \n",
       "10                              642                  16   \n",
       "11                              642                  14   \n",
       "12                              642                  15   \n",
       "13                              642                   7   \n",
       "14                              642                  12   \n",
       "15                              642                  32   \n",
       "16                              642                   8   \n",
       "17                              474                  17   \n",
       "18                              474                  11   \n",
       "19                              474                  21   \n",
       "20                              635                  31   \n",
       "21                              474                  18   \n",
       "22                              474                  20   \n",
       "23                              474                  15   \n",
       "24                              474                  14   \n",
       "25                              474                  12   \n",
       "26                              474                   9   \n",
       "27                              474                  15   \n",
       "28                              474                  18   \n",
       "29                              474                  18   \n",
       "30                              474                  20   \n",
       "31                              474                  17   \n",
       "32                             2326                   5   \n",
       "33                             2957                 103   \n",
       "34                              635                  17   \n",
       "35                              444                   2   \n",
       "36                              635                  46   \n",
       "\n",
       "                                              example  \n",
       "0   （, textbook, h₂o, satisfy, trajectory, julio, ...  \n",
       "1   00, adversary, intended, ambiguous, cooked, た,...  \n",
       "2   203, ?, [SEP], ロ, game, 640, り, victories, cal...  \n",
       "3   ', ;, contestants, accidental, ɔ, continents, ...  \n",
       "4   ĠThere, There, Ġthere, </s>, there, ĠTHERE, ĠE...  \n",
       "5   Discussion, ĠâĢĭ, ĠSubjects, Topic, Ġ?, .-, Ġs...  \n",
       "6   </s>, lucrarea, ▁grains, ▁photographed, ▁sport...  \n",
       "7   </s>, ▁»., <extra_id_27>, ▁Comment, ▁Ribbon, c...  \n",
       "8   </s>, <extra_id_0>, <extra_id_27>, ▁velvet, ▁c...  \n",
       "9   </s>, ▁consacré, <extra_id_27>, ▁hashtag, ▁hel...  \n",
       "10  </s>, lucrarea, ▁Someone, <extra_id_26>, ▁happ...  \n",
       "11  ▁»., </s>, <extra_id_27>, <extra_id_25>, ▁supp...  \n",
       "12  </s>, <extra_id_0>, <extra_id_9>, <extra_id_27...  \n",
       "13  </s>, ▁consacré, ▁shortly, Pourtant, ▁indeed, ...  \n",
       "14  </s>, lucrarea, <extra_id_26>, ▁somewhere, <ex...  \n",
       "15  </s>, ▁»., <extra_id_27>, ▁waiting, ▁exhausted...  \n",
       "16  </s>, <extra_id_0>, <extra_id_9>, <extra_id_27...  \n",
       "17  [SEP], exhibiting, occurring, pretended, behav...  \n",
       "18  generating, absorbing, heating, carpet, human,...  \n",
       "19  ರ, ⇄, 扌, [SEP], ∅, ⺩, [MASK], ⺼, [PAD], 都, [CL...  \n",
       "20  ▁sont, ▁peut, ▁много, жду, ▁испо, ▁которы, ци,...  \n",
       "21  [, brought, ð, deposited, december, climax, ex...  \n",
       "22  neighbouring, ？, witnessed, granting, 。, proce...  \n",
       "23  actively, intended, intercepted, intentional, ...  \n",
       "24  [SEP], ɔ, ո, occurring, having, intercept, ʊ, ...  \n",
       "25  [CLS], [MASK], ¦, polling, 勝, [SEP], qualifier...  \n",
       "26  [CLS], [MASK], [SEP], cerambycidae, ～, etienne...  \n",
       "27  [SEP], [CLS], treacherous, 2nd, peacefully, 水,...  \n",
       "28  [SEP], [MASK], hotspur, [CLS], aroused, 3a, mo...  \n",
       "29  ٹ, 1st, 30th, mcgrath, rendering, 15th, ɑ, 33r...  \n",
       "30  [CLS], ~, ₆, ₎, ,, ₍, ∞, ₃, ■, ⊕, ⁴, ⇌, ᄌ, ℓ, ...  \n",
       "31  扌, multiplied, ː, ∧, ʑ, ‿, ♯, ^, factual, ɪ, h...  \n",
       "32    Ġthru, Ġgifted, Ġupfront, Ġportraying, Ġawkward  \n",
       "33  Ġanon, Ġcommenting, Ġsolver, ĠChecking, ĠSteer...  \n",
       "34  ▁adventures, ▁promoting, ▁nine, ▁folks, ▁villa...  \n",
       "35                                  zeichnet, ▁scales  \n",
       "36  ▁которы, ▁годи, ▁Jahrhund, ▁который, ▁которых,...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\n",
    "    \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\",\n",
    "    \"sup-simcse-bert-base-uncased\", \"sup-simcse-bert-large-uncased\", \"sup-simcse-roberta-base\", \"sup-simcse-roberta-large\",\n",
    "    \"sentence-t5-base\", \"sentence-t5-large\", \"sentence-t5-xl\", \"sentence-t5-xxl\",\n",
    "    \"gtr-t5-base\", \"gtr-t5-large\", \"gtr-t5-xl\", \"gtr-t5-xxl\",\n",
    "    \"instructor-base\", \"instructor-large\", \"instructor-xl\",\n",
    "    \"e5-small\", \"e5-base\", \"e5-large\", \"e5-mistral-7b-instruct\",\n",
    "    \"bge-small-en-v1.5\", \"bge-base-en-v1.5\", \"bge-large-en-v1.5\",\n",
    "    \"UAE-Large-V1\",\n",
    "    \"nomic-embed-text-v1\", \"nomic-embed-text-v1.5\",\n",
    "    \"gte-small\", \"gte-base\", \"gte-large\", \"gte-base-en-v1.5\", \"gte-large-en-v1.5\", \"gte-Qwen2-1.5B-instruct\", \"gte-Qwen2-7B-instruct\",\n",
    "    \"GritLM-7B\",\n",
    "    \"SFR-Embedding-2_R\", \"SFR-Embedding-Mistral\",\n",
    "]\n",
    "# 按照model_names的顺序对DataFrame进行排序\n",
    "experiement_record_df = experiement_record_df.set_index('model_name').loc[model_names].reset_index()\n",
    "my_df = experiement_record_df[['model_name', 'num_parameters', 'vocab_size', 'ok_tokens_num', 'candidates_for_verification_num', 'sticky token count']].assign(\n",
    "    num_parameters=lambda x: (x['num_parameters'] / 1e6).round().astype(int).astype(str) + 'M',\n",
    "    sticky_token_count=lambda x: x['sticky token count'].astype(int),  # 将sticky token count转换为整数\n",
    "    example=lambda x: x['model_name'].apply(lambda name: ', '.join(get_sticky_token_list(name)))\n",
    ").drop(columns=['sticky token count'])  # 删除原来的sticky token count列\n",
    "my_df.to_latex(buf=\"../results/final_sticky_token_table.tex\", index=False)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv(\"sticky_tokens_cross_model.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('</s>', 12),\n",
       " ('[SEP]', 9),\n",
       " ('<extra_id_27>', 8),\n",
       " ('[CLS]', 7),\n",
       " ('<extra_id_19>', 5),\n",
       " ('<extra_id_0>', 4),\n",
       " ('[MASK]', 4),\n",
       " ('lucrarea', 3),\n",
       " ('<extra_id_18>', 3),\n",
       " ('▁».', 3),\n",
       " ('intended', 3),\n",
       " ('[PAD]', 3),\n",
       " ('<extra_id_12>', 3),\n",
       " ('<extra_id_9>', 3),\n",
       " ('？', 2),\n",
       " ('having', 2),\n",
       " ('deposited', 2),\n",
       " ('▁hashtag', 2),\n",
       " ('occurring', 2),\n",
       " ('behaved', 2),\n",
       " ('⺩', 2),\n",
       " ('勝', 2),\n",
       " ('扌', 2),\n",
       " ('▁somebody', 2),\n",
       " ('▁Someone', 2),\n",
       " ('<extra_id_26>', 2),\n",
       " ('momentarily', 2),\n",
       " ('<extra_id_25>', 2),\n",
       " ('<extra_id_13>', 2),\n",
       " ('▁indeed', 2),\n",
       " ('▁consacré', 2),\n",
       " ('ɔ', 2),\n",
       " ('▁которы', 2),\n",
       " ('ющи', 2),\n",
       " ('▁voegen', 2),\n",
       " ('▁най', 2),\n",
       " ('▁отри', 2),\n",
       " ('▁tip', 1),\n",
       " ('▁br', 1),\n",
       " ('▁Gas', 1),\n",
       " ('▁blanc', 1),\n",
       " ('▁organism', 1),\n",
       " ('▁brake', 1),\n",
       " ('▁joke', 1),\n",
       " ('▁Gate', 1),\n",
       " ('durant', 1),\n",
       " ('▁Tablet', 1),\n",
       " ('▁grains', 1),\n",
       " ('▁sportive', 1),\n",
       " ('▁Portable', 1),\n",
       " ('▁Patio', 1),\n",
       " ('▁pastel', 1),\n",
       " ('▁meme', 1),\n",
       " ('▁photographed', 1),\n",
       " ('▁Hose', 1),\n",
       " ('▁cum', 1),\n",
       " ('▁chance', 1),\n",
       " ('▁sharing', 1),\n",
       " ('▁Comment', 1),\n",
       " ('▁Cum', 1),\n",
       " ('▁trigger', 1),\n",
       " ('▁Brook', 1),\n",
       " ('▁cod', 1),\n",
       " ('▁cald', 1),\n",
       " ('▁pixels', 1),\n",
       " ('▁candid', 1),\n",
       " ('▁groove', 1),\n",
       " ('▁hue', 1),\n",
       " ('▁Medal', 1),\n",
       " ('prayed', 1),\n",
       " ('▁Cardinal', 1),\n",
       " ('▁poat', 1),\n",
       " ('stroke', 1),\n",
       " ('brushed', 1),\n",
       " ('▁melting', 1),\n",
       " ('▁pahar', 1),\n",
       " ('▁buckle', 1),\n",
       " ('cliquez', 1),\n",
       " ('▁tactile', 1),\n",
       " ('▁tripod', 1),\n",
       " ('▁Ribbon', 1),\n",
       " ('▁bumbac', 1),\n",
       " ('{', 1),\n",
       " ('₂', 1),\n",
       " ('う', 1),\n",
       " ('（', 1),\n",
       " ('intelligent', 1),\n",
       " ('functioning', 1),\n",
       " ('satisfy', 1),\n",
       " ('cultivated', 1),\n",
       " ('defendant', 1),\n",
       " ('julio', 1),\n",
       " ('textbook', 1),\n",
       " ('functioned', 1),\n",
       " ('trajectory', 1),\n",
       " ('h₂o', 1),\n",
       " ('whereupon', 1),\n",
       " ('gambia', 1),\n",
       " ('imaginative', 1),\n",
       " ('oskar', 1),\n",
       " ('。', 1),\n",
       " ('goal', 1),\n",
       " ('threatened', 1),\n",
       " ('indicates', 1),\n",
       " ('www', 1),\n",
       " ('permitted', 1),\n",
       " ('encountered', 1),\n",
       " ('proceeded', 1),\n",
       " ('neighbouring', 1),\n",
       " ('witnessed', 1),\n",
       " ('investigated', 1),\n",
       " ('detected', 1),\n",
       " ('granting', 1),\n",
       " ('transporting', 1),\n",
       " ('bordering', 1),\n",
       " ('positioning', 1),\n",
       " ('witnessing', 1),\n",
       " ('progressing', 1),\n",
       " ('[', 1),\n",
       " ('ð', 1),\n",
       " ('december', 1),\n",
       " ('brought', 1),\n",
       " ('exposed', 1),\n",
       " ('absorbed', 1),\n",
       " ('assembled', 1),\n",
       " ('horizontal', 1),\n",
       " ('m³', 1),\n",
       " ('climax', 1),\n",
       " ('encourages', 1),\n",
       " ('hinted', 1),\n",
       " ('fastened', 1),\n",
       " ('influenza', 1),\n",
       " ('membranes', 1),\n",
       " ('т', 1),\n",
       " ('х', 1),\n",
       " ('←', 1),\n",
       " ('⇒', 1),\n",
       " ('「', 1),\n",
       " ('た', 1),\n",
       " ('な', 1),\n",
       " ('治', 1),\n",
       " ('november', 1),\n",
       " ('00', 1),\n",
       " ('shouted', 1),\n",
       " ('democracy', 1),\n",
       " ('tobacco', 1),\n",
       " ('abortion', 1),\n",
       " ('cooked', 1),\n",
       " ('intending', 1),\n",
       " ('uzbekistan', 1),\n",
       " ('truce', 1),\n",
       " ('qaeda', 1),\n",
       " ('ambiguous', 1),\n",
       " ('betray', 1),\n",
       " ('alerted', 1),\n",
       " ('adversary', 1),\n",
       " ('▁images', 1),\n",
       " ('▁source', 1),\n",
       " ('▁album', 1),\n",
       " ('▁context', 1),\n",
       " ('▁sauce', 1),\n",
       " ('▁PDF', 1),\n",
       " ('▁trained', 1),\n",
       " ('▁Instagram', 1),\n",
       " ('▁caught', 1),\n",
       " ('▁Victoria', 1),\n",
       " ('▁attitude', 1),\n",
       " ('▁floral', 1),\n",
       " ('▁notification', 1),\n",
       " ('▁pomp', 1),\n",
       " ('▁vibr', 1),\n",
       " ('▁grammar', 1),\n",
       " ('▁Swift', 1),\n",
       " ('▁velvet', 1),\n",
       " ('LAR', 1),\n",
       " ('▁Alert', 1),\n",
       " ('▁collage', 1),\n",
       " ('▁Proof', 1),\n",
       " ('▁Sauce', 1),\n",
       " ('▁Candida', 1),\n",
       " ('▁Carson', 1),\n",
       " ('▁Vince', 1),\n",
       " ('▁strig', 1),\n",
       " ('▁Disclaimer', 1),\n",
       " ('suddenly', 1),\n",
       " ('expecting', 1),\n",
       " ('clenched', 1),\n",
       " ('pretending', 1),\n",
       " ('pretended', 1),\n",
       " ('campaigned', 1),\n",
       " ('gleaming', 1),\n",
       " ('chewed', 1),\n",
       " ('cerebral', 1),\n",
       " ('makeshift', 1),\n",
       " ('belongings', 1),\n",
       " ('reelected', 1),\n",
       " ('thereof', 1),\n",
       " ('exhibiting', 1),\n",
       " ('human', 1),\n",
       " ('whoever', 1),\n",
       " ('carpet', 1),\n",
       " ('heating', 1),\n",
       " ('generating', 1),\n",
       " ('absorbing', 1),\n",
       " ('glide', 1),\n",
       " ('harvesting', 1),\n",
       " ('ochreous', 1),\n",
       " ('vibrating', 1),\n",
       " ('craving', 1),\n",
       " ('ರ', 1),\n",
       " ('ར', 1),\n",
       " ('ნ', 1),\n",
       " ('ᄃ', 1),\n",
       " ('⇄', 1),\n",
       " ('∅', 1),\n",
       " ('⺼', 1),\n",
       " ('⽥', 1),\n",
       " ('ね', 1),\n",
       " ('力', 1),\n",
       " ('皇', 1),\n",
       " ('都', 1),\n",
       " ('面', 1),\n",
       " ('circumstance', 1),\n",
       " ('attended', 1),\n",
       " ('continue', 1),\n",
       " ('participated', 1),\n",
       " ('maintaining', 1),\n",
       " ('actively', 1),\n",
       " ('lately', 1),\n",
       " ('interactions', 1),\n",
       " ('issuing', 1),\n",
       " ('intercepted', 1),\n",
       " ('int', 1),\n",
       " ('intentional', 1),\n",
       " ('uploaded', 1),\n",
       " ('asserting', 1),\n",
       " ('exercising', 1),\n",
       " ('▁happened', 1),\n",
       " ('▁facing', 1),\n",
       " ('incredibly', 1),\n",
       " ('▁somehow', 1),\n",
       " ('▁Something', 1),\n",
       " ('▁Certain', 1),\n",
       " ('notably', 1),\n",
       " ('▁Schaden', 1),\n",
       " ('³', 1),\n",
       " ('а', 1),\n",
       " ('ि', 1),\n",
       " ('₀', 1),\n",
       " ('jaime', 1),\n",
       " ('betrayal', 1),\n",
       " ('tossing', 1),\n",
       " ('aroused', 1),\n",
       " ('ulrich', 1),\n",
       " ('3a', 1),\n",
       " ('hotspur', 1),\n",
       " ('harassed', 1),\n",
       " ('impromptu', 1),\n",
       " ('水', 1),\n",
       " ('21', 1),\n",
       " ('2nd', 1),\n",
       " ('8th', 1),\n",
       " ('civilian', 1),\n",
       " ('midnight', 1),\n",
       " ('presently', 1),\n",
       " ('peacefully', 1),\n",
       " ('dripped', 1),\n",
       " ('tolerant', 1),\n",
       " ('treacherous', 1),\n",
       " (',', 1),\n",
       " ('>', 1),\n",
       " ('~', 1),\n",
       " ('ᄌ', 1),\n",
       " ('⁴', 1),\n",
       " ('₃', 1),\n",
       " ('₆', 1),\n",
       " ('₍', 1),\n",
       " ('₎', 1),\n",
       " ('ℓ', 1),\n",
       " ('⇌', 1),\n",
       " ('∞', 1),\n",
       " ('∩', 1),\n",
       " ('⊕', 1),\n",
       " ('■', 1),\n",
       " ('立', 1),\n",
       " ('龸', 1),\n",
       " ('▁problem', 1),\n",
       " ('▁doch', 1),\n",
       " ('▁Problem', 1),\n",
       " ('▁supposed', 1),\n",
       " ('▁zis', 1),\n",
       " ('problem', 1),\n",
       " ('▁reicht', 1),\n",
       " ('▁Erwachsene', 1),\n",
       " ('ɑ', 1),\n",
       " ('ٹ', 1),\n",
       " ('1st', 1),\n",
       " ('unusual', 1),\n",
       " ('15th', 1),\n",
       " ('ninth', 1),\n",
       " ('twentieth', 1),\n",
       " ('laying', 1),\n",
       " ('30th', 1),\n",
       " ('rendering', 1),\n",
       " ('posting', 1),\n",
       " ('scratched', 1),\n",
       " ('33rd', 1),\n",
       " ('tilting', 1),\n",
       " ('mcgrath', 1),\n",
       " ('45th', 1),\n",
       " ('00pm', 1),\n",
       " ('55th', 1),\n",
       " ('^', 1),\n",
       " ('ɪ', 1),\n",
       " ('ʑ', 1),\n",
       " ('ˈ', 1),\n",
       " ('ː', 1),\n",
       " ('ᵒ', 1),\n",
       " ('ᵗ', 1),\n",
       " ('‰', 1),\n",
       " ('‿', 1),\n",
       " ('∈', 1),\n",
       " ('∧', 1),\n",
       " ('♯', 1),\n",
       " ('happens', 1),\n",
       " ('probable', 1),\n",
       " ('factual', 1),\n",
       " ('multiplied', 1),\n",
       " ('▁really', 1),\n",
       " ('▁quite', 1),\n",
       " ('▁truly', 1),\n",
       " ('alleged', 1),\n",
       " ('▁badly', 1),\n",
       " ('tocmai', 1),\n",
       " ('supposedly', 1),\n",
       " ('▁staggering', 1),\n",
       " ('▁young', 1),\n",
       " ('▁according', 1),\n",
       " ('▁village', 1),\n",
       " ('▁accident', 1),\n",
       " ('▁photo', 1),\n",
       " ('▁nine', 1),\n",
       " ('▁folks', 1),\n",
       " ('▁publication', 1),\n",
       " ('▁junior', 1),\n",
       " ('▁©', 1),\n",
       " ('▁elite', 1),\n",
       " ('▁islands', 1),\n",
       " ('▁promoting', 1),\n",
       " ('▁’', 1),\n",
       " ('▁escort', 1),\n",
       " ('▁accidents', 1),\n",
       " ('▁adventures', 1),\n",
       " ('▁friend', 1),\n",
       " ('▁massage', 1),\n",
       " ('▁placeholder', 1),\n",
       " ('▁Massage', 1),\n",
       " ('▁hello', 1),\n",
       " ('pictured', 1),\n",
       " ('▁whistle', 1),\n",
       " ('▁charcoal', 1),\n",
       " ('▁faceti', 1),\n",
       " ('▁sparkle', 1),\n",
       " ('▁temptation', 1),\n",
       " ('▁oyster', 1),\n",
       " ('▁Ferguson', 1),\n",
       " ('▁shimmer', 1),\n",
       " ('▁eyebrow', 1),\n",
       " ('▁hyperlink', 1),\n",
       " ('viction', 1),\n",
       " ('▁underscore', 1),\n",
       " ('?', 1),\n",
       " ('ʌ', 1),\n",
       " ('ր', 1),\n",
       " ('ر', 1),\n",
       " ('り', 1),\n",
       " ('ろ', 1),\n",
       " ('ロ', 1),\n",
       " ('・', 1),\n",
       " ('game', 1),\n",
       " ('dr', 1),\n",
       " ('calling', 1),\n",
       " ('62', 1),\n",
       " ('agree', 1),\n",
       " ('andrea', 1),\n",
       " ('victories', 1),\n",
       " ('yield', 1),\n",
       " ('eduardo', 1),\n",
       " ('ren', 1),\n",
       " ('203', 1),\n",
       " ('640', 1),\n",
       " ('vigor', 1),\n",
       " (\"'\", 1),\n",
       " (';', 1),\n",
       " (']', 1),\n",
       " ('president', 1),\n",
       " ('whatever', 1),\n",
       " ('contestants', 1),\n",
       " ('contestant', 1),\n",
       " ('differently', 1),\n",
       " ('accidental', 1),\n",
       " ('continents', 1),\n",
       " ('Ġthere', 1),\n",
       " ('ĠâĢĶ', 1),\n",
       " ('ĠThere', 1),\n",
       " ('There', 1),\n",
       " ('Ġhappening', 1),\n",
       " ('ÂŃ', 1),\n",
       " ('there', 1),\n",
       " ('ĠÂŃ', 1),\n",
       " ('.]', 1),\n",
       " ('.âĢĶ', 1),\n",
       " ('ĠNotably', 1),\n",
       " ('Ġ]', 1),\n",
       " ('ĠTHERE', 1),\n",
       " ('âĢİ', 1),\n",
       " ('ĠEdit', 1),\n",
       " ('âĢĲ', 1),\n",
       " ('âĢĵâĢĵ', 1),\n",
       " ('ĠâĪĴ', 1),\n",
       " ('Ġï¿½', 1),\n",
       " ('Specifically', 1),\n",
       " ('][', 1),\n",
       " ('_.', 1),\n",
       " ('âĢ¦]', 1),\n",
       " ('[]', 1),\n",
       " ('Ġtion', 1),\n",
       " ('=]', 1),\n",
       " ('Ġsubject', 1),\n",
       " ('.-', 1),\n",
       " ('Ġreferring', 1),\n",
       " ('Source', 1),\n",
       " ('Ġcommented', 1),\n",
       " ('ĠâĢĭ', 1),\n",
       " ('Ġ?', 1),\n",
       " ('ĠCaption', 1),\n",
       " ('About', 1),\n",
       " ('.).', 1),\n",
       " ('Currently', 1),\n",
       " ('ĠSubject', 1),\n",
       " ('subject', 1),\n",
       " ('Description', 1),\n",
       " ('description', 1),\n",
       " ('?\".', 1),\n",
       " ('Ġnoun', 1),\n",
       " ('Ġschematic', 1),\n",
       " ('ĠSubjects', 1),\n",
       " ('Anyway', 1),\n",
       " ('){', 1),\n",
       " ('Discussion', 1),\n",
       " ('Topic', 1),\n",
       " (')].', 1),\n",
       " ('CONCLUS', 1),\n",
       " ('▁strongly', 1),\n",
       " ('▁shortly', 1),\n",
       " ('▁briefly', 1),\n",
       " ('Pourtant', 1),\n",
       " ('ра', 1),\n",
       " ('ци', 1),\n",
       " ('ду', 1),\n",
       " ('▁ч', 1),\n",
       " ('▁ст', 1),\n",
       " ('▁sont', 1),\n",
       " ('▁был', 1),\n",
       " ('бли', 1),\n",
       " ('ць', 1),\n",
       " ('▁peut', 1),\n",
       " ('▁släktet', 1),\n",
       " ('жду', 1),\n",
       " ('▁испо', 1),\n",
       " ('пол', 1),\n",
       " ('▁ию', 1),\n",
       " ('ват', 1),\n",
       " ('цу', 1),\n",
       " ('▁klikken', 1),\n",
       " ('ця', 1),\n",
       " ('чо', 1),\n",
       " ('▁много', 1),\n",
       " ('стов', 1),\n",
       " ('мож', 1),\n",
       " ('▁постро', 1),\n",
       " ('▁honom', 1),\n",
       " ('ץ', 1),\n",
       " ('》', 1),\n",
       " ('行', 1),\n",
       " ('～', 1),\n",
       " ('challenging', 1),\n",
       " ('cerambycidae', 1),\n",
       " ('etienne', 1),\n",
       " ('¦', 1),\n",
       " ('ᆼ', 1),\n",
       " ('₉', 1),\n",
       " ('ₖ', 1),\n",
       " ('appearing', 1),\n",
       " ('qualifier', 1),\n",
       " ('polling', 1),\n",
       " ('tipping', 1),\n",
       " ('▁observ', 1),\n",
       " ('▁satisf', 1),\n",
       " ('▁насе', 1),\n",
       " ('▁годи', 1),\n",
       " ('▁вре', 1),\n",
       " ('▁уча', 1),\n",
       " ('▁нача', 1),\n",
       " ('▁occas', 1),\n",
       " ('▁Jahrhund', 1),\n",
       " ('▁райо', 1),\n",
       " ('▁algun', 1),\n",
       " ('▁окру', 1),\n",
       " ('▁явля', 1),\n",
       " ('▁того', 1),\n",
       " ('▁чемпи', 1),\n",
       " ('▁получи', 1),\n",
       " ('▁который', 1),\n",
       " ('▁распо', 1),\n",
       " ('ześ', 1),\n",
       " ('▁района', 1),\n",
       " ('▁llev', 1),\n",
       " ('▁DCHECK', 1),\n",
       " ('▁trabaj', 1),\n",
       " ('▁побе', 1),\n",
       " ('▁режи', 1),\n",
       " ('▁которые', 1),\n",
       " ('▁соста', 1),\n",
       " ('▁изда', 1),\n",
       " ('▁актив', 1),\n",
       " ('▁них', 1),\n",
       " ('▁участи', 1),\n",
       " ('▁befindet', 1),\n",
       " ('▁служ', 1),\n",
       " ('▁invån', 1),\n",
       " ('▁участ', 1),\n",
       " ('▁invånare', 1),\n",
       " ('▁furono', 1),\n",
       " ('▁которых', 1),\n",
       " ('▁смер', 1),\n",
       " ('▁занима', 1),\n",
       " ('▁той', 1),\n",
       " ('zeichnet', 1),\n",
       " ('▁scales', 1),\n",
       " ('▁or', 1),\n",
       " ('▁there', 1),\n",
       " ('▁outside', 1),\n",
       " ('▁somewhere', 1),\n",
       " ('<extra_id_22>', 1),\n",
       " ('Ġgraph', 1),\n",
       " ('Ġdating', 1),\n",
       " ('Ġfix', 1),\n",
       " ('Ġsubs', 1),\n",
       " ('Ġoil', 1),\n",
       " ('Ġnotes', 1),\n",
       " ('Ġopinion', 1),\n",
       " ('Ġcoding', 1),\n",
       " ('Ġpassing', 1),\n",
       " ('Ġchecks', 1),\n",
       " ('Ġfiled', 1),\n",
       " ('Ġflavor', 1),\n",
       " ('Ġanswered', 1),\n",
       " ('Ġreaching', 1),\n",
       " ('Ġresolved', 1),\n",
       " ('Ġbanned', 1),\n",
       " ('Ġpointing', 1),\n",
       " ('Ġbay', 1),\n",
       " ('Ġforums', 1),\n",
       " ('Ġshar', 1),\n",
       " ('ĠMining', 1),\n",
       " ('Ġbump', 1),\n",
       " ('Ġbaking', 1),\n",
       " ('Ġcomics', 1),\n",
       " ('Ġenters', 1),\n",
       " ('Ġsolver', 1),\n",
       " ('Ġvegan', 1),\n",
       " ('Ġlemon', 1),\n",
       " ('Ġpeg', 1),\n",
       " ('Ġoils', 1),\n",
       " ('Ġmusicians', 1),\n",
       " ('Ġpix', 1),\n",
       " ('ĠHindu', 1),\n",
       " ('Ġtagged', 1),\n",
       " ('Ġbee', 1),\n",
       " ('Ġtho', 1),\n",
       " ('Ġcommenting', 1),\n",
       " ('Ġdib', 1),\n",
       " ('Ġmarble', 1),\n",
       " ('Ġlime', 1),\n",
       " ('ĠElectrical', 1),\n",
       " ('Ġmilling', 1),\n",
       " ('ĠChecking', 1),\n",
       " ('Ġdecoding', 1),\n",
       " ('Ġtint', 1),\n",
       " ('Ġbidding', 1),\n",
       " ('ĠTHC', 1),\n",
       " ('Ġbaz', 1),\n",
       " ('Ġtrending', 1),\n",
       " ('Ġtally', 1),\n",
       " ('ĠFishing', 1),\n",
       " ('Ġyrs', 1),\n",
       " ('Ġbeads', 1),\n",
       " ('Ġjpeg', 1),\n",
       " ('Ġnumb', 1),\n",
       " ('Ġreel', 1),\n",
       " ('Ġpaging', 1),\n",
       " ('Ġthumbs', 1),\n",
       " ('Ġtaxable', 1),\n",
       " ('ĠSeeking', 1),\n",
       " ('ĠPuzzle', 1),\n",
       " ('Ġdetox', 1),\n",
       " ('Ġlumber', 1),\n",
       " ('Ġclr', 1),\n",
       " ('Ġrepent', 1),\n",
       " ('Ġhail', 1),\n",
       " ('Ġcreek', 1),\n",
       " ('ĠHearing', 1),\n",
       " ('ĠTide', 1),\n",
       " ('Ġkettle', 1),\n",
       " ('ĠCandle', 1),\n",
       " ('ĠDangerous', 1),\n",
       " ('Ġoptimizing', 1),\n",
       " ('ĠElectricity', 1),\n",
       " ('Ġlax', 1),\n",
       " ('ĠFacial', 1),\n",
       " ('Ġanon', 1),\n",
       " ('ĠMeasurements', 1),\n",
       " ('ĠSHR', 1),\n",
       " ('ĠSteering', 1),\n",
       " ('Ġthrott', 1),\n",
       " ('Ġmeddling', 1),\n",
       " ('Ġcorrecting', 1),\n",
       " ('ĠCooling', 1),\n",
       " ('Ġcovid', 1),\n",
       " ('Ġpolled', 1),\n",
       " ('Ġblinking', 1),\n",
       " ('Ġmultiplying', 1),\n",
       " ('Ġbreakup', 1),\n",
       " ('ĠFollowers', 1),\n",
       " ('Ġsire', 1),\n",
       " ('ĠOnion', 1),\n",
       " ('Ġiterating', 1),\n",
       " ('ĠCameras', 1),\n",
       " ('Ġspotting', 1),\n",
       " ('Ġtimeouts', 1),\n",
       " ('Ġcds', 1),\n",
       " ('Ġbev', 1),\n",
       " ('ĠAsking', 1),\n",
       " ('Ġshl', 1),\n",
       " ('Ġbarber', 1),\n",
       " ('Ġsyncing', 1),\n",
       " ('Ġiam', 1),\n",
       " ('Ġawkward', 1),\n",
       " ('Ġthru', 1),\n",
       " ('Ġgifted', 1),\n",
       " ('Ġupfront', 1),\n",
       " ('Ġportraying', 1),\n",
       " ('ɛ', 1),\n",
       " ('ʊ', 1),\n",
       " ('ε', 1),\n",
       " ('і', 1),\n",
       " ('ո', 1),\n",
       " ('א', 1),\n",
       " ('を', 1),\n",
       " ('equipped', 1),\n",
       " ('posted', 1),\n",
       " ('intercept', 1),\n",
       " ('▁newly', 1),\n",
       " ('▁apparently', 1),\n",
       " ('▁during', 1),\n",
       " ('▁several', 1),\n",
       " ('▁moment', 1),\n",
       " ('▁wieder', 1),\n",
       " ('▁significant', 1),\n",
       " ('▁waiting', 1),\n",
       " ('▁meine', 1),\n",
       " ('▁hundreds', 1),\n",
       " ('▁moments', 1),\n",
       " ('▁happening', 1),\n",
       " ('▁turning', 1),\n",
       " ('▁Again', 1),\n",
       " ('▁unexpected', 1),\n",
       " ('▁ending', 1),\n",
       " ('▁suddenly', 1),\n",
       " ('▁finishing', 1),\n",
       " ('▁sudden', 1),\n",
       " ('▁Congratulations', 1),\n",
       " ('▁urine', 1),\n",
       " ('▁exhausted', 1),\n",
       " ('▁laughter', 1),\n",
       " ('▁sunglasses', 1),\n",
       " ('▁penetrate', 1),\n",
       " ('misunderstanding', 1),\n",
       " ('manipulated', 1),\n",
       " ('<extra_id_5>', 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "all_verified_tokens = pd.read_csv(\"../results/final_all_models_sticky_tokens.csv\")[\"raw_vocab\"].to_list()\n",
    "token_counts = Counter(all_verified_tokens)\n",
    "sorted_token_counts = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_token_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stickytoken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
